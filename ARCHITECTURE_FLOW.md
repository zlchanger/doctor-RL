# DoctorAgent-RL åˆ†å¸ƒå¼è®­ç»ƒæ¶æ„è¯¦è§£

> æœ¬æ–‡æ¡£è¯¦ç»†è§£é‡Šä½ æåˆ°çš„æ¶æ„å›¾åœ¨ä»£ç ä¸­çš„å…·ä½“å®ç°ä½ç½®

---

## æ¶æ„å›¾å›é¡¾

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Ray åˆ†å¸ƒå¼åè°ƒå™¨    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          â”‚          â”‚          â”‚          â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”‚
   â”‚Actor    â”‚ â”‚Critic â”‚ â”‚Rolloutâ”‚ â”‚Env LLM   â”‚  â”‚
   â”‚Worker   â”‚ â”‚Worker â”‚ â”‚Worker â”‚ â”‚Worker    â”‚  â”‚
   â”‚(FSDP)   â”‚ â”‚(PPO)  â”‚ â”‚       â”‚ â”‚(vLLM)    â”‚  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚         â”‚          â”‚          â”‚          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    è®­ç»ƒå¾ªç¯æµç¨‹
```

---

## ä¸€ã€æ¶æ„ç»„ä»¶ä»£ç å®šä½

### 1. Ray åˆ†å¸ƒå¼åè°ƒå™¨

**æ ¸å¿ƒæ–‡ä»¶**: `ragen/trainer/main_ppo.py`

#### 1.1 å…¥å£å‡½æ•° (main_ppo.py:167-173)

```python
@hydra.main(config_path='config', config_name='ppo_trainer', version_base=None)
def main(config):
    if not ray.is_initialized():
        # åˆå§‹åŒ–æœ¬åœ°Rayé›†ç¾¤
        ray.init(runtime_env={'env_vars': {'TOKENIZERS_PARALLELISM': 'true', 'NCCL_DEBUG': 'WARN'}})

    # é€šè¿‡Rayè¿œç¨‹æ‰§è¡Œä¸»ä»»åŠ¡
    ray.get(main_task.remote(config))
```

**è¯´æ˜**:
- ä½¿ç”¨ Hydra åŠ è½½é…ç½®
- åˆå§‹åŒ– Ray é›†ç¾¤
- å¯åŠ¨è¿œç¨‹ä»»åŠ¡ `main_task`

#### 1.2 ä¸»ä»»åŠ¡å‡½æ•° (main_ppo.py:177-270)

```python
@ray.remote
def main_task(config):
    # 1. åŠ è½½tokenizer (main_ppo.py:194)
    tokenizer = hf_tokenizer(local_path)

    # 2. å®šä¹‰Workerç±»å‹æ˜ å°„ (main_ppo.py:215-233)
    role_worker_mapping = {
        Role.ActorRollout: ray.remote(ActorRolloutRefWorker),  # Actor + Rollout åˆå¹¶
        Role.Critic: ray.remote(CriticWorker),                 # Critic Worker
        Role.RefPolicy: ray.remote(ActorRolloutRefWorker),     # Reference Policy
        Role.EnvLLM: ray.remote(EnvironmentLLMWorker),         # ç¯å¢ƒLLM (Patient)
    }

    # 3. å®šä¹‰èµ„æºæ±  (main_ppo.py:221-229)
    resource_pool_spec = {
        'global_pool': [n_gpus_per_node] * nnodes,  # ä¾‹å¦‚: [8, 8] è¡¨ç¤º2ä¸ªèŠ‚ç‚¹,æ¯ä¸ª8ä¸ªGPU
    }
    mapping = {
        Role.ActorRollout: 'global_pool',
        Role.Critic: 'global_pool',
        Role.RefPolicy: 'global_pool',
        Role.EnvLLM: 'global_pool',
    }

    # 4. åˆ›å»ºè®­ç»ƒå™¨ (main_ppo.py:259-268)
    trainer = RayPPOTrainer(
        config=config,
        tokenizer=tokenizer,
        role_worker_mapping=role_worker_mapping,
        resource_pool_manager=ResourcePoolManager(resource_pool_spec, mapping),
        reward_fn=reward_fn,
        env=train_env,
    )

    # 5. åˆå§‹åŒ–Workerså¹¶å¼€å§‹è®­ç»ƒ (main_ppo.py:269-270)
    trainer.init_workers()  # åˆ†é…GPUèµ„æº,åˆ›å»ºWorkerè¿›ç¨‹
    trainer.fit()           # å¼€å§‹è®­ç»ƒå¾ªç¯
```

---

### 2. Worker åˆå§‹åŒ–æµç¨‹

**æ ¸å¿ƒæ–‡ä»¶**: `ragen/trainer/ppo/ray_trainer.py`

#### 2.1 RayPPOTrainer ç±»å®šä¹‰ (ray_trainer.py:399-478)

```python
class RayPPOTrainer(object):
    """
    åœ¨Driverè¿›ç¨‹(å•ä¸ªCPU/GPUèŠ‚ç‚¹)ä¸Šè¿è¡Œçš„è®­ç»ƒå™¨
    """

    def __init__(self, config, tokenizer, role_worker_mapping, resource_pool_manager, ...):
        self.config = config
        self.tokenizer = tokenizer
        self.role_worker_mapping = role_worker_mapping  # Role -> Workerç±»çš„æ˜ å°„
        self.resource_pool_manager = resource_pool_manager

        # æ ¹æ®ç®—æ³•é€‰æ‹©æ˜¯å¦ä½¿ç”¨Critic
        if config.algorithm.adv_estimator == 'gae':
            self.use_critic = True   # PPOä½¿ç”¨Critic
        elif config.algorithm.adv_estimator == 'grpo':
            self.use_critic = False  # GRPOä¸éœ€è¦Critic
```

#### 2.2 init_workers() æ–¹æ³• (ray_trainer.py:686-773)

è¿™æ˜¯**æ ¸å¿ƒæ–¹æ³•**,åˆ›å»ºæ‰€æœ‰Workerå®ä¾‹:

```python
def init_workers(self):
    """
    åˆ›å»ºæ‰€æœ‰Workerç»„,åˆ†é…GPUèµ„æº
    """
    # 1. åˆ›å»ºèµ„æºæ± 
    self.resource_pool_manager.create_resource_pool()

    # 2. ä¸ºæ¯ä¸ªRoleåˆ›å»ºWorkerç±»é…ç½®
    self.resource_pool_to_cls = defaultdict(dict)

    # 2.1 åˆ›å»º ActorRollout Worker (ray_trainer.py:695-703)
    resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout)
    actor_rollout_cls = RayClassWithInitArgs(
        cls=self.role_worker_mapping[Role.ActorRollout],
        config=self.config.actor_rollout_ref,
        role='actor_rollout'
    )
    self.resource_pool_to_cls[resource_pool]['actor_rollout'] = actor_rollout_cls

    # 2.2 åˆ›å»º Critic Worker (å¦‚æœéœ€è¦) (ray_trainer.py:705-709)
    if self.use_critic:
        resource_pool = self.resource_pool_manager.get_resource_pool(Role.Critic)
        critic_cls = RayClassWithInitArgs(
            cls=self.role_worker_mapping[Role.Critic],
            config=self.config.critic
        )
        self.resource_pool_to_cls[resource_pool]["critic"] = critic_cls

    # 2.3 åˆ›å»º Reference Policy Worker (ray_trainer.py:711-717)
    if self.use_reference_policy:
        resource_pool = self.resource_pool_manager.get_resource_pool(Role.RefPolicy)
        ref_policy_cls = RayClassWithInitArgs(
            self.role_worker_mapping[Role.RefPolicy],
            config=self.config.actor_rollout_ref,
            role='ref'
        )
        self.resource_pool_to_cls[resource_pool]['ref'] = ref_policy_cls

    # 2.4 åˆ›å»º Env LLM Worker (Patientæ¨¡æ‹Ÿ) (ray_trainer.py:726-732)
    if self.config.env.use_env_llm:
        resource_pool = self.resource_pool_manager.get_resource_pool(Role.EnvLLM)
        env_llm_cls = RayClassWithInitArgs(
            self.role_worker_mapping[Role.EnvLLM],
            config=self.config.env.env_llm,
            role='env_llm'
        )
        self.resource_pool_to_cls[resource_pool]['env_llm'] = env_llm_cls

    # 3. åˆå§‹åŒ– WorkerGroup (ray_trainer.py:738-773)
    all_wg = {}
    for resource_pool, class_dict in self.resource_pool_to_cls.items():
        # åˆ›å»ºè”åˆWorkerç±» (å¤šä¸ªWorkerå…±äº«GPU)
        worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)

        # åˆ›å»º RayWorkerGroup
        wg_dict = self.ray_worker_group_cls(
            resource_pool=resource_pool,
            ray_cls_with_init=worker_dict_cls
        )

        # ç”ŸæˆWorkerå®ä¾‹
        spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())
        all_wg.update(spawn_wg)

    # 4. åˆå§‹åŒ–å„ä¸ªWorkerçš„æ¨¡å‹
    if self.use_critic:
        self.critic_wg = all_wg['critic']
        self.critic_wg.init_model()  # åŠ è½½Criticæ¨¡å‹

    if self.use_reference_policy:
        self.ref_policy_wg = all_wg['ref']
        self.ref_policy_wg.init_model()  # åŠ è½½Referenceæ¨¡å‹

    if self.config.env.use_env_llm:
        self.env_llm_wg = all_wg['env_llm']
        self.env_llm_wg.init_model()  # åŠ è½½Patient LLM (vLLM)
        self.env.env_llm_worker = self.env_llm_wg  # æ³¨å…¥åˆ°ç¯å¢ƒ

    # Actor + Rollout Workeræœ€ååˆå§‹åŒ– (è®©vLLMå…ˆå ç”¨æ˜¾å­˜)
    self.actor_rollout_wg = all_wg['actor_rollout']
    self.actor_rollout_wg.init_model()  # åŠ è½½Actoræ¨¡å‹ (FSDP)
```

**å…³é”®ç‚¹**:
- `create_colocated_worker_cls`: å…è®¸å¤šä¸ªWorkerå…±äº«åŒä¸€ç»„GPUèµ„æº
- åˆå§‹åŒ–é¡ºåºå¾ˆé‡è¦: EnvLLMå…ˆåˆå§‹åŒ–(vLLMéœ€è¦é¢„ç•™æ˜¾å­˜), ActorRolloutæœ€ååˆå§‹åŒ–

---

### 3. è®­ç»ƒå¾ªç¯æµç¨‹

**æ ¸å¿ƒæ–‡ä»¶**: `ragen/trainer/ppo/ray_trainer.py`

#### 3.1 fit() æ–¹æ³• - ä¸»è®­ç»ƒå¾ªç¯ (ray_trainer.py:904-1199)

```python
def fit(self):
    """
    PPOè®­ç»ƒå¾ªç¯
    Driverè¿›ç¨‹é€šè¿‡RPCè°ƒç”¨Workerçš„è®¡ç®—å‡½æ•°æ¥æ„å»ºPPOæ•°æ®æµ
    è½»é‡çº§çš„advantageè®¡ç®—åœ¨Driverè¿›ç¨‹å®Œæˆ
    """

    # å‡†å¤‡Generationé…ç½®
    gen_config = GenerationConfig(
        max_turns=self.config.max_turns,  # æœ€å¤§å¯¹è¯è½®æ¬¡
        max_response_length=self.config.data.max_response_length,
        # ...
    )

    # åˆ›å»ºLLMç”Ÿæˆç®¡ç†å™¨
    generation_manager = LLMGenerationManager(
        tokenizer=self.tokenizer,
        actor_rollout_wg=self.actor_rollout_wg,  # Actor Worker
        env_class=self.env_class,
        config=gen_config,
    )

    # ä¸ºæ¯ä¸ªpromptåˆ›å»ºç‹¬ç«‹çš„ç¯å¢ƒå®ä¾‹
    envs = [self.env.copy() for _ in range(batch_size * n_agent)]

    # =========================================
    # å¼€å§‹è®­ç»ƒå¾ªç¯
    # =========================================
    for epoch in range(total_epochs):
        for batch_dict in self.train_dataloader:

            # ====================================
            # æ­¥éª¤1: ç¯å¢ƒé‡ç½® (ray_trainer.py:976-982)
            # ====================================
            batch = DataProto.from_single_dict(batch_dict)
            batch = batch.repeat(repeat_times=n_agent, interleave=True)

            env_seeds = [item['index'] for item in batch.non_tensor_batch['extra_info']]
            for env, seed in zip(envs, env_seeds):
                env.reset(seed=seed)  # é‡ç½®ç¯å¢ƒ,åŠ è½½æ‚£è€…æ¡ˆä¾‹

            # ====================================
            # æ­¥éª¤2: Rollout - å¤šè½®å¯¹è¯ç”Ÿæˆ (ray_trainer.py:1007-1031)
            # ====================================
            gen_batch = batch.pop(batch_keys=['input_ids', 'attention_mask', 'position_ids'])

            # è¿è¡Œå¤šè½®å¯¹è¯å¾ªç¯
            final_gen_batch_output = generation_manager.run_llm_loop(
                gen_batch=gen_batch,
                envs=envs,                    # ç¯å¢ƒåˆ—è¡¨ (Patientæ¨¡æ‹Ÿ)
                initial_input_ids=first_input_ids,
                output_dir=output_dir,
                global_steps=self.global_steps,
            )
            # è¿™ä¸ªå¾ªç¯å†…éƒ¨:
            # for turn in range(max_turns):
            #     1. Actorç”Ÿæˆé—®é¢˜/è¯Šæ–­
            #     2. æå–<answer>æ ‡ç­¾å†…å®¹
            #     3. è°ƒç”¨env.step(action) -> è®¡ç®—reward
            #     4. å¦‚æœæœªç»“æŸ, EnvLLMç”Ÿæˆæ‚£è€…å›ç­”
            #     5. æ‹¼æ¥å¯¹è¯å†å²,ç»§ç»­ä¸‹ä¸€è½®

            # ====================================
            # æ­¥éª¤3: æ”¶é›†Rewardå’ŒMetrics (ray_trainer.py:1037-1084)
            # ====================================
            # è®¾ç½®uidç”¨äºGRPOåˆ†ç»„å½’ä¸€åŒ–
            if adv_estimator == 'grpo':
                batch.non_tensor_batch['uid'] = np.array([str(i) for i in env_seeds])

            # æ”¶é›†æ¯ä¸ªepisodeçš„reward
            batch.non_tensor_batch['reward'] = np.array([env.reward for env in envs])

            # æ”¶é›†è¯Šæ–­å¾—åˆ† (å¦‚æœç¯å¢ƒæ”¯æŒ)
            if hasattr(envs[0], 'diagnosis_score'):
                batch.non_tensor_batch['diagnosis_score'] = [env.diagnosis_score for env in envs]

            # Rewardå½’ä¸€åŒ– (GRPO/BRPO/ARPO)
            if reward_norm_type is not None:
                batch.non_tensor_batch['reward'] = normalize_reward(
                    batch.non_tensor_batch['reward'],
                    batch.non_tensor_batch['uid'],
                    reward_norm_type
                )

            # æ”¶é›†actionç»Ÿè®¡
            for idx, env in enumerate(envs):
                tracking_vars = env.get_tracking_variables()
                batch.non_tensor_batch['valid_action'][idx] = sum(1 for x in tracking_vars['actions_valid'] if x is not None)
                batch.non_tensor_batch['effective_action'][idx] = sum(1 for x in tracking_vars['actions_effective'] if x is not None)

            # ====================================
            # æ­¥éª¤4: è®¡ç®—Log Probs (ray_trainer.py:1104-1116)
            # ====================================
            # ä½¿ç”¨å½“å‰ç­–ç•¥é‡æ–°è®¡ç®—log_prob (ç”¨äºPPOæ›´æ–°)
            old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)
            batch = batch.union(old_log_prob)

            # ====================================
            # æ­¥éª¤5: è®¡ç®—Reference Log Probs (ray_trainer.py:1118-1122)
            # ====================================
            if self.use_reference_policy:
                ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)
                batch = batch.union(ref_log_prob)

            # ====================================
            # æ­¥éª¤6: è®¡ç®—Values (ä»…PPOéœ€è¦) (ray_trainer.py:1124-1128)
            # ====================================
            if self.use_critic:
                values = self.critic_wg.compute_values(batch)
                batch = batch.union(values)

            # ====================================
            # æ­¥éª¤7: è®¡ç®—Advantages (ray_trainer.py:1130-1157)
            # ====================================
            # åº”ç”¨KLæƒ©ç½š (å¦‚æœä½¿ç”¨)
            if use_kl_in_reward:
                batch, kl_metrics = apply_kl_penalty(batch, kl_ctrl=self.kl_ctrl_in_reward)
            else:
                batch.batch["token_level_rewards"] = batch.batch["token_level_scores"]

            # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
            batch = compute_advantage(
                batch,
                adv_estimator=self.config.algorithm.adv_estimator,  # 'grpo' or 'gae'
                gamma=self.config.algorithm.gamma,
                lam=self.config.algorithm.lam,
                num_repeat=n_agent
            )

            # ====================================
            # æ­¥éª¤8: æ›´æ–°Critic (ä»…PPO) (ray_trainer.py:1159-1164)
            # ====================================
            if self.use_critic:
                critic_output = self.critic_wg.update_critic(batch)
                metrics.update(critic_output.meta_info['metrics'])

            # ====================================
            # æ­¥éª¤9: æ›´æ–°Actor (ray_trainer.py:1167-1174)
            # ====================================
            if self.global_steps > critic_warmup:
                actor_output = self.actor_rollout_wg.update_actor(batch)
                metrics.update(actor_output.meta_info['metrics'])

            # ====================================
            # æ­¥éª¤10: éªŒè¯å’Œä¿å­˜ (ray_trainer.py:1176-1186)
            # ====================================
            if self.global_steps % test_freq == 0:
                val_metrics = self._validate()
                metrics.update(val_metrics)

            if self.global_steps % save_freq == 0:
                self._save_checkpoint()

            # è®°å½•æŒ‡æ ‡
            logger.log(data=metrics, step=self.global_steps)
            self.global_steps += 1
```

---

## äºŒã€æ ¸å¿ƒæ•°æ®æµ

### 1. Rollouté˜¶æ®µçš„å¤šè½®å¯¹è¯æµç¨‹

**å…³é”®æ–‡ä»¶**: `ragen/llm_agent/generation.py` (LLMGenerationManager)

```python
class LLMGenerationManager:
    def run_llm_loop(self, gen_batch, envs, initial_input_ids, ...):
        """
        è¿è¡Œå¤šè½®å¯¹è¯å¾ªç¯
        """
        for turn_idx in range(max_turns):
            # 1. Actorç”Ÿæˆå›ç­” (é—®é¢˜æˆ–è¯Šæ–­)
            gen_output = self.actor_rollout_wg.generate_sequences(current_gen_batch)

            # 2. è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            responses = self.tokenizer.batch_decode(gen_output.batch['responses'])

            # 3. æå–<answer>æ ‡ç­¾å†…å®¹
            actions = [extract_answer(resp) for resp in responses]

            # 4. ç¯å¢ƒæ‰§è¡Œaction,è®¡ç®—reward
            observations = []
            for env, action in zip(envs, actions):
                obs, reward, done, info = env.step(action)
                observations.append(obs)

            # 5. å¦‚æœepisodeæœªç»“æŸ,ç”Ÿæˆæ‚£è€…å›ç­”
            if not all_done:
                # è°ƒç”¨ EnvLLM Worker (vLLM)
                patient_responses = self.env_llm_wg.generate_patient_response(
                    patient_descriptions=descriptions,
                    dialogue_histories=histories
                )

                # 6. æ‹¼æ¥å¯¹è¯å†å²
                for idx, patient_resp in enumerate(patient_responses):
                    histories[idx] += f"\nDoctor: {actions[idx]}\nPatient: {patient_resp}"

            # 7. å¦‚æœæ‰€æœ‰episodeéƒ½ç»“æŸ,é€€å‡ºå¾ªç¯
            if all([env.finished() for env in envs]):
                break

        return final_gen_batch_output
```

### 2. ç¯å¢ƒ (Environment) çš„æ‰§è¡Œæµç¨‹

**å…³é”®æ–‡ä»¶**: `ragen/env/medical_consultation/env_patient_llm.py`

```python
class MedicalConsultationEnvWithPatientLLM:
    def reset(self, seed):
        """
        åˆå§‹åŒ–episode
        """
        self.index = self._shared_seed_to_index[seed]
        self.description = self._shared_data[self.index]['description']  # æ‚£è€…æè¿°
        self.target_diagnosis = self._shared_data[self.index]['target']  # æ­£ç¡®è¯Šæ–­
        self.patient_information = self._shared_data[self.index]['patient_information']  # æœ‰æ•ˆé—®ç­”å¯¹

        self.current_turn = 0
        self.diagnosis_made = False
        self.reward = 0.0

        # è¿”å›åˆå§‹è§‚å¯Ÿ
        return self.description

    def step(self, action):
        """
        æ‰§è¡Œdoctorçš„action (é—®é¢˜æˆ–è¯Šæ–­)
        """
        self.current_turn += 1

        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯è¯Šæ–­
        if self._is_diagnosis(action):
            self.diagnosis_made = True

            # è®¡ç®—è¯Šæ–­å¥–åŠ±
            if self._check_diagnosis_correct(action, self.target_diagnosis):
                self.reward += 1.0  # è¯Šæ–­æ­£ç¡®
            else:
                self.reward += 0.0  # è¯Šæ–­é”™è¯¯

            done = True
            obs = "Diagnosis submitted."

        # 2. å¦åˆ™æ˜¯æé—®
        else:
            # æ£€æŸ¥é—®é¢˜æ˜¯å¦æœ‰æ•ˆ (åœ¨patient_informationä¸­)
            is_valid = self._check_question_valid(action, self.patient_information)

            if is_valid:
                self.reward += 0.1  # æœ‰æ•ˆé—®é¢˜å¥–åŠ±
                self._actions_valid.append(True)
            else:
                self.reward += -0.05  # æ— æ•ˆé—®é¢˜æƒ©ç½š
                self._actions_valid.append(False)

            # å¯¹è¯è½®æ¬¡æƒ©ç½š
            self.reward += -0.01

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ¬¡
            done = (self.current_turn >= self.max_turns)

            # è§‚å¯Ÿå°†ç”±EnvLLMç”Ÿæˆ (æ‚£è€…å›ç­”)
            obs = None  # å°†åœ¨å¤–éƒ¨é€šè¿‡env_llm_workerå¡«å……

        return obs, self.reward, done, {}

    def _check_question_valid(self, question, patient_information):
        """
        æ£€æŸ¥é—®é¢˜æ˜¯å¦åœ¨æœ‰æ•ˆé—®ç­”å¯¹åˆ—è¡¨ä¸­
        """
        for qa_pair in patient_information:
            doctor_q = qa_pair['doctor_question']
            # ä½¿ç”¨ROUGEåˆ†æ•°æˆ–è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­
            if similarity(question, doctor_q) > threshold:
                return True
        return False
```

---

## ä¸‰ã€Worker å†…éƒ¨å®ç°

### 1. ActorRollout Worker (FSDPç­–ç•¥æ¨¡å‹)

**å…³é”®æ–‡ä»¶**: `ragen/workers/fsdp_workers.py` (æˆ– `verl/workers/fsdp_workers.py`)

```python
class ActorRolloutRefWorker:
    def init_model(self):
        """
        åˆå§‹åŒ–FSDPæ¨¡å‹
        """
        # åŠ è½½HuggingFaceæ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(model_path)

        # åŒ…è£…ä¸ºFSDP
        self.model = FSDP(
            self.model,
            sharding_strategy=ShardingStrategy.FULL_SHARD,
            # ...
        )

    def generate_sequences(self, batch):
        """
        Rolloutç”Ÿæˆ (é‡‡æ ·å¤šä¸ªå›ç­”ç”¨äºGRPO)
        """
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=batch.batch['input_ids'],
                attention_mask=batch.batch['attention_mask'],
                max_new_tokens=max_response_length,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
                num_return_sequences=n_agent,  # GRPO: æ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›ç­”
            )

        return DataProto(batch={'responses': outputs})

    def compute_log_prob(self, batch):
        """
        è®¡ç®—å½“å‰ç­–ç•¥çš„log_prob
        """
        with torch.no_grad():
            logits = self.model(
                input_ids=batch.batch['input_ids'],
                attention_mask=batch.batch['attention_mask']
            ).logits

            log_probs = compute_log_probs_from_logits(logits, batch.batch['responses'])

        return DataProto(batch={'old_log_probs': log_probs})

    def update_actor(self, batch):
        """
        PPO/GRPOç­–ç•¥æ›´æ–°
        """
        for micro_batch in split_into_micro_batches(batch):
            # å‰å‘ä¼ æ’­
            logits = self.model(
                input_ids=micro_batch.batch['input_ids'],
                attention_mask=micro_batch.batch['attention_mask']
            ).logits

            # è®¡ç®—PPO loss
            loss = compute_ppo_loss(
                logits=logits,
                old_log_probs=micro_batch.batch['old_log_probs'],
                advantages=micro_batch.batch['advantages'],
                clip_ratio=self.config.clip_ratio
            )

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦ç´¯ç§¯åæ›´æ–°
            if (step + 1) % gradient_accumulation_steps == 0:
                self.optimizer.step()
                self.optimizer.zero_grad()

        return DataProto(meta_info={'metrics': {'actor/loss': loss.item()}})
```

### 2. Env LLM Worker (vLLMæ‚£è€…æ¨¡æ‹Ÿ)

**å…³é”®æ–‡ä»¶**: `ragen/workers/env_llm_worker.py`

```python
class EnvironmentLLMWorker:
    def init_model(self):
        """
        åˆå§‹åŒ–vLLMæ¨ç†å¼•æ“
        """
        from vllm import LLM, SamplingParams

        self.llm = LLM(
            model=self.config.model_path,
            tensor_parallel_size=self.config.tp_size,
            gpu_memory_utilization=0.85,
            # vLLMé«˜æ€§èƒ½æ¨ç†é…ç½®
        )

        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=self.config.max_tokens
        )

    def generate_patient_response(self, prompts):
        """
        æ‰¹é‡ç”Ÿæˆæ‚£è€…å›ç­”

        Args:
            prompts: List[str], æ¯ä¸ªpromptåŒ…å«:
                - æ‚£è€…æè¿°
                - å¯¹è¯å†å²
                - åŒ»ç”Ÿæœ€æ–°é—®é¢˜

        Returns:
            List[str]: æ‚£è€…å›ç­”
        """
        # ä½¿ç”¨vLLMæ‰¹é‡æ¨ç†
        outputs = self.llm.generate(prompts, self.sampling_params)

        # æå–ç”Ÿæˆçš„æ–‡æœ¬
        responses = [output.outputs[0].text for output in outputs]

        return responses
```

**Promptæ ¼å¼ç¤ºä¾‹**:
```
ä½ æ˜¯ä¸€ä½æ‚£è€…,æ ¹æ®ä»¥ä¸‹æè¿°å›ç­”åŒ»ç”Ÿçš„é—®é¢˜:

æ‚£è€…æè¿°:
{patient_description}

å¯¹è¯å†å²:
Doctor: æ‚¨å¥½,è¯·é—®å“ªé‡Œä¸èˆ’æœ?
Patient: æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿåˆ°å£æ¸´,è¿˜ç»å¸¸ä¸Šå•æ‰€ã€‚

Doctor: {latest_question}
Patient:
```

### 3. Critic Worker (ä»…PPOéœ€è¦)

**å…³é”®æ–‡ä»¶**: `ragen/workers/fsdp_workers.py`

```python
class CriticWorker:
    def init_model(self):
        """
        åˆå§‹åŒ–Valueç½‘ç»œ
        """
        # é€šå¸¸åŸºäºActoræ¨¡å‹,æ›¿æ¢æœ€åä¸€å±‚ä¸ºå•è¾“å‡º
        self.value_model = AutoModelForSequenceClassification.from_pretrained(
            model_path,
            num_labels=1  # è¾“å‡ºå•ä¸ªvalue
        )

        # FSDPåŒ…è£…
        self.value_model = FSDP(self.value_model, ...)

    def compute_values(self, batch):
        """
        è®¡ç®—state value
        """
        with torch.no_grad():
            values = self.value_model(
                input_ids=batch.batch['input_ids'],
                attention_mask=batch.batch['attention_mask']
            ).logits.squeeze(-1)

        return DataProto(batch={'values': values})

    def update_critic(self, batch):
        """
        æ›´æ–°Valueç½‘ç»œ
        """
        # è®¡ç®—target value (ä½¿ç”¨GAE)
        target_values = batch.batch['advantages'] + batch.batch['values']

        # å‰å‘ä¼ æ’­
        predicted_values = self.value_model(...).logits.squeeze(-1)

        # MSE loss
        loss = F.mse_loss(predicted_values, target_values)

        loss.backward()
        self.optimizer.step()

        return DataProto(meta_info={'metrics': {'critic/loss': loss.item()}})
```

---

## å››ã€å…³é”®ç®—æ³•å®ç°

### 1. GRPOä¼˜åŠ¿å‡½æ•°è®¡ç®—

**æ–‡ä»¶**: `ragen/trainer/ppo/ray_trainer.py:145-200`

```python
def compute_advantage(data: DataProto, adv_estimator, gamma=1.0, lam=1.0, num_repeat=1):
    """
    è®¡ç®—ä¼˜åŠ¿å‡½æ•°

    Args:
        data: åŒ…å«rewards, uid (ç”¨äºåˆ†ç»„)
        adv_estimator: 'grpo' | 'gae' | 'brpo' | ...
        num_repeat: æ¯ä¸ªpromptçš„å“åº”æ•° (GRPOç”¨)
    """

    if adv_estimator == 'grpo':
        # Group Relative Policy Optimization
        # 1. æŒ‰uidåˆ†ç»„
        grouped_rewards = defaultdict(list)
        for uid, reward in zip(data.non_tensor_batch['uid'], data.non_tensor_batch['reward']):
            grouped_rewards[uid].append(reward)

        # 2. ç»„å†…å½’ä¸€åŒ–
        advantages = []
        for uid in data.non_tensor_batch['uid']:
            group_rewards = grouped_rewards[uid]
            # å‡å»ç»„å†…å‡å€¼,é™¤ä»¥ç»„å†…æ ‡å‡†å·®
            mean = np.mean(group_rewards)
            std = np.std(group_rewards) + 1e-8
            adv = (data.non_tensor_batch['reward'] - mean) / std
            advantages.append(adv)

        data.batch['advantages'] = torch.tensor(advantages)

    elif adv_estimator == 'gae':
        # Generalized Advantage Estimation (PPO)
        # A_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)^2 Î´_{t+2} + ...
        # å…¶ä¸­ Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)

        values = data.batch['values']
        rewards = data.batch['token_level_rewards']

        advantages = []
        last_gae = 0
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + gamma * values[t+1] - values[t]
            last_gae = delta + gamma * lam * last_gae
            advantages.insert(0, last_gae)

        data.batch['advantages'] = torch.tensor(advantages)

    return data
```

### 2. PPO Lossè®¡ç®—

**æ–‡ä»¶**: `verl/trainer/ppo/core_algos.py`

```python
def compute_ppo_loss(logits, old_log_probs, advantages, clip_ratio=0.2):
    """
    PPO Clipped Objective

    L^CLIP(Î¸) = E[ min(r(Î¸)A, clip(r(Î¸), 1-Îµ, 1+Îµ)A) ]
    å…¶ä¸­ r(Î¸) = Ï€_Î¸(a|s) / Ï€_old(a|s)
    """
    # è®¡ç®—å½“å‰ç­–ç•¥çš„log_prob
    log_probs = compute_log_probs_from_logits(logits, actions)

    # è®¡ç®—ratio
    ratio = torch.exp(log_probs - old_log_probs)

    # è®¡ç®—ä¸¤ä¸ªç›®æ ‡
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio) * advantages

    # å–æœ€å°å€¼å¹¶æ±‚è´Ÿ (å› ä¸ºæˆ‘ä»¬æœ€å¤§åŒ–ç›®æ ‡)
    loss = -torch.min(surr1, surr2).mean()

    return loss
```

---

## äº”ã€å®Œæ•´è®­ç»ƒæµç¨‹æ€»ç»“

### è®­ç»ƒä¸€ä¸ªBatchçš„å®Œæ•´æ­¥éª¤:

```
1. æ•°æ®åŠ è½½ (Driver)
   â”œâ”€ ä»DataLoaderè·å–batch (æ‚£è€…æ¡ˆä¾‹)
   â””â”€ é‡å¤n_agentæ¬¡ (GRPOéœ€è¦æ¯ä¸ªpromptå¤šä¸ªå“åº”)

2. ç¯å¢ƒé‡ç½® (Driver)
   â”œâ”€ ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºç¯å¢ƒå®ä¾‹
   â””â”€ env.reset(seed) - åŠ è½½æ‚£è€…æè¿°ã€æ­£ç¡®è¯Šæ–­ç­‰

3. Rollout - å¤šè½®å¯¹è¯ç”Ÿæˆ (Actor Worker + Env LLM Worker)
   â”œâ”€ for turn in range(max_turns):
   â”‚   â”œâ”€ Actorç”Ÿæˆ: è°ƒç”¨ actor_rollout_wg.generate_sequences()
   â”‚   â”œâ”€ æå–action: è§£æ <answer>æ ‡ç­¾
   â”‚   â”œâ”€ ç¯å¢ƒæ‰§è¡Œ: env.step(action) -> reward, done
   â”‚   â”œâ”€ å¦‚æœæœªç»“æŸ:
   â”‚   â”‚   â”œâ”€ æ„é€ Patient Prompt (æè¿° + å†å² + é—®é¢˜)
   â”‚   â”‚   â”œâ”€ Env LLMç”Ÿæˆæ‚£è€…å›ç­”: env_llm_wg.generate()
   â”‚   â”‚   â””â”€ æ‹¼æ¥å¯¹è¯å†å²
   â”‚   â””â”€ å¦‚æœæ‰€æœ‰episodeç»“æŸ -> break
   â””â”€ è¿”å›å®Œæ•´å¯¹è¯å†å² + rewards

4. æ”¶é›†Metrics (Driver)
   â”œâ”€ ä»ç¯å¢ƒæ”¶é›†: reward, diagnosis_score, valid_actions
   â””â”€ Rewardå½’ä¸€åŒ– (GRPO/BRPO)

5. è®¡ç®—Log Probs (Actor Worker)
   â”œâ”€ å½“å‰ç­–ç•¥: actor_rollout_wg.compute_log_prob()
   â””â”€ å‚è€ƒç­–ç•¥: ref_policy_wg.compute_ref_log_prob() [å¯é€‰]

6. è®¡ç®—Values (Critic Worker) [ä»…PPO]
   â””â”€ critic_wg.compute_values()

7. è®¡ç®—Advantages (Driver)
   â”œâ”€ åº”ç”¨KLæƒ©ç½š [å¯é€‰]
   â””â”€ æ ¹æ®ç®—æ³•è®¡ç®—:
       â”œâ”€ GRPO: ç»„å†…å½’ä¸€åŒ–
       â””â”€ GAE: æ—¶åºå·®åˆ†ä¼°è®¡

8. æ›´æ–°Critic (Critic Worker) [ä»…PPO]
   â””â”€ critic_wg.update_critic() - MSE loss

9. æ›´æ–°Actor (Actor Worker)
   â””â”€ actor_rollout_wg.update_actor() - PPO/GRPO loss

10. éªŒè¯å’Œä¿å­˜ (Driver + Actor Worker)
    â”œâ”€ å®šæœŸéªŒè¯: _validate()
    â””â”€ å®šæœŸä¿å­˜: _save_checkpoint()
```

---

## å…­ã€é…ç½®æ–‡ä»¶å¯¹åº”å…³ç³»

### å…³é”®é…ç½®å‚æ•° -> ä»£ç ä½ç½®

| é…ç½®å‚æ•° | ä»£ç ä½ç½® | è¯´æ˜ |
|---------|---------|------|
| `data.train_batch_size` | `ray_trainer.py:976` | æ¯æ¬¡rolloutçš„promptæ•° |
| `actor_rollout_ref.rollout.n_agent` | `ray_trainer.py:977` | æ¯ä¸ªpromptçš„å“åº”æ•°(GRPO) |
| `algorithm.adv_estimator` | `ray_trainer.py:451-462` | 'grpo' æˆ– 'gae' |
| `env.max_turns` | `generation.py` | æœ€å¤§å¯¹è¯è½®æ¬¡ (-1ä¸ºéšæœº2-10) |
| `env.use_env_llm` | `ray_trainer.py:727-732` | æ˜¯å¦ä½¿ç”¨vLLMæ¨¡æ‹Ÿæ‚£è€… |
| `actor_rollout_ref.actor.ppo_mini_batch_size` | `fsdp_workers.py` | PPOæ›´æ–°batchå¤§å° |
| `algorithm.kl_ctrl.kl_coef` | `ray_trainer.py:114-143` | KLæ•£åº¦æƒ©ç½šç³»æ•° |
| `trainer.n_gpus_per_node` | `main_ppo.py:223` | æ¯ä¸ªèŠ‚ç‚¹GPUæ•° |
| `trainer.nnodes` | `main_ppo.py:223` | èŠ‚ç‚¹æ•° |

---

## ä¸ƒã€è°ƒè¯•å’Œç›‘æ§

### 1. æŸ¥çœ‹Workeråˆ†é…

```bash
# å¯åŠ¨è®­ç»ƒå,å¯ä»¥åœ¨Ray DashboardæŸ¥çœ‹
# http://localhost:8265

# æˆ–ä½¿ç”¨å‘½ä»¤è¡Œ
ray status
```

### 2. å…³é”®æ—¥å¿—ä½ç½®

- **è®­ç»ƒæŒ‡æ ‡**: WandB (å¦‚æœé…ç½®äº†WANDB_API_KEY)
- **Workeræ—¥å¿—**: Rayä¼šè‡ªåŠ¨é‡å®šå‘åˆ° `/tmp/ray/session_*/logs/`
- **Checkpoint**: `checkpoints/{project_name}/{exp_name}/global_step_{N}/`

### 3. å¸¸è§è°ƒè¯•ç‚¹

```python
# åœ¨ ray_trainer.py:1007 æ·»åŠ æ–­ç‚¹,æŸ¥çœ‹rolloutè¾“å…¥
# åœ¨ env_patient_llm.py:84 æŸ¥çœ‹ç¯å¢ƒé‡ç½®
# åœ¨ generation.py æŸ¥çœ‹å¤šè½®å¯¹è¯ç”Ÿæˆè¿‡ç¨‹
```

---

## å…«ã€æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. GPUæ˜¾å­˜åˆ†é…ç­–ç•¥

```python
# ray_trainer.py:771-773
# Actoråœ¨æœ€ååˆå§‹åŒ–,è®©vLLMå…ˆå ç”¨æ˜¾å­˜
self.env_llm_wg.init_model()  # vLLMå…ˆåˆå§‹åŒ–
self.actor_rollout_wg.init_model()  # Actorååˆå§‹åŒ–
```

### 2. æ‰¹é‡å¤§å°è°ƒä¼˜

```yaml
# æ€»æœ‰æ•ˆbatchå¤§å° = train_batch_size * n_agent * gradient_accumulation_steps
data.train_batch_size: 8       # Rolloutæ‰¹é‡
actor_rollout_ref.rollout.n_agent: 4  # GRPOå“åº”æ•°
actor_rollout_ref.actor.gradient_accumulation_steps: 2

# å®é™…æ¯æ¬¡æ›´æ–°çš„æ ·æœ¬æ•° = 8 * 4 * 2 = 64
```

### 3. vLLMå¹¶è¡Œåº¦

```yaml
env.env_llm.tensor_parallel_size: 2  # vLLMä½¿ç”¨2ä¸ªGPUå¹¶è¡Œ
env.env_llm.gpu_memory_utilization: 0.85
```

---

## æ€»ç»“

è¿™ä¸ªæ¶æ„å›¾åœ¨ä»£ç ä¸­çš„å¯¹åº”å…³ç³»:

1. **Rayåˆ†å¸ƒå¼åè°ƒå™¨** â†’ `main_ppo.py` + `ray_trainer.py:RayPPOTrainer`
2. **Actor Worker** â†’ `fsdp_workers.py:ActorRolloutRefWorker`
3. **Critic Worker** â†’ `fsdp_workers.py:CriticWorker`
4. **Rollout Worker** â†’ ä¸Actoråˆå¹¶åœ¨ `ActorRolloutRefWorker`
5. **Env LLM Worker** â†’ `env_llm_worker.py:EnvironmentLLMWorker`
6. **è®­ç»ƒå¾ªç¯æµç¨‹** â†’ `ray_trainer.py:fit()` (ç¬¬904-1199è¡Œ)

**æ ¸å¿ƒè®­ç»ƒæµç¨‹**: ç¯å¢ƒé‡ç½® â†’ å¤šè½®Rollout â†’ æ”¶é›†Reward â†’ è®¡ç®—Advantage â†’ æ›´æ–°ç­–ç•¥

å¸Œæœ›è¿™ä¸ªè¯¦ç»†çš„æ–‡æ¡£èƒ½å¸®åŠ©ä½ ç†è§£æ•´ä¸ªåˆ†å¸ƒå¼è®­ç»ƒæ¶æ„! ğŸš€