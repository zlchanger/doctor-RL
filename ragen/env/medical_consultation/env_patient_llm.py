from typing import Optional, List, Tuple, Any, Dict
from ragen.env.base import BaseLanguageBasedEnv
import random
import gymnasium as gym
import pandas as pd
import re
from rouge_score import rouge_scorer
import json
import torch
from verl import DataProto
from verl.utils.model import compute_position_id_with_mask
from transformers import AutoTokenizer
from ragen.workers.env_llm_worker import EnvironmentLLMWorker
from omegaconf import OmegaConf
import ray
import numpy as np
from ragen.env.medical_consultation.env import MedicalConsultationEnv

class MedicalConsultationEnvWithPatientLLM(MedicalConsultationEnv):
    """
    A medical consultation environment where a doctor (policy model) interacts with a patient (fixed LLM).
    The doctor asks questions and makes a diagnosis, while the patient responds based on their condition.
    The patient's responses are generated by an external LLM and passed to the environment.
    """

    PENALTY_FOR_INVALID = 0

    def __init__(self, parquet_path: str, env_llm_worker=None, tokenizer=None, max_turns=5):
        """
        Initialize the environment for Medical Consultation

        Args:
            parquet_path: Path to the parquet file containing patient data
            env_llm_worker: Worker for generating patient responses
            tokenizer: Tokenizer for the environment LLM
        """
        if max_turns == -1:
            # random select a turn from 2 to 10
            self.random_turn = True
        else:
            self.random_turn = False
        super().__init__(parquet_path, env_llm_worker, tokenizer, max_turns)
        self.description = None
        self.diagnosis_score = None
        self.recommandation_score = None

    @staticmethod
    def _get_data_from_parquet(path: str):
        """
        Get data from parquet file and create mapping.

        Args:
            path: Path to the parquet file containing the data

        Returns:
            tuple: (data, mapping) where
                data: List of dicts containing target and numbers for each problem
                mapping: Dict mapping original indices to new sequential indices

        The function:
        1. Reads the parquet file
        2. Extracts target numbers and available numbers for each problem
        3. Creates an index mapping to maintain reference to original data
        """
        df = pd.read_parquet(path)
        
        # Extract target and numbers for each problem
        data = []
        for item in df.reward_model.values:
            data.append(
                {
                    'target': item['ground_truth'],
                    'patient_information': item['patient_information'],
                    'description': item['enhanced_description'],
                }
            )

        # Create mapping from original indices to sequential indices
        original_indices = [item['index'] for item in df.extra_info.values]
        seed_to_index = {orig_idx: new_idx for new_idx, orig_idx in enumerate(original_indices)}
        
        return data, seed_to_index
    
    def reset(self, seed: int = None) -> str:
        """Reset the environment and reward distributions"""
        gym.Env.reset(self, seed=seed)

        if self.random_turn:
            # random select a turn from 2 to 10
            self.max_turns = random.randint(2, 10) # TODO: make it configurable

        # Reset tracking variables
        self._reset_tracking_variables()
        self.index = self._shared_seed_to_index[seed]
        # self.candidate_questions = self._shared_data[self.index]['patient_information']
        self.description = self._shared_data[self.index]['description']
        # 过滤一下self.candidate_questions，保留'patient_response'和'doctor_question'同时存在的
        # self.candidate_questions = [q for q in self.candidate_questions if "patient_response" in q and "doctor_question" in q and isinstance(q['patient_response'], np.ndarray) and isinstance(q['doctor_question'], np.ndarray)]
        # self.visited_questions = []  # index of the visited questions
        self.diagnosis_made = False
        self.current_turn = 0
        self.conversation_history = []
        self.diagnosis_score = 0
        self.recommandation_score = 0
        return self.render()

    def _prepare_judge_prompt(self, doctor_question):
        """
        Prepare a prompt for the environment LLM based on the doctor's question.
        
        Args:
            doctor_question: The doctor's question
            
        Returns:
            A formatted prompt for the environment LLM
        """
        history_questions = []
        for i, conv in enumerate(self.conversation_history):
            if conv['role'] == 'doctor' and "</answer>" not in conv['content']:
                if ":" in conv['content']:
                    history_questions.append(f"Q{i}. {conv['content'].split(':')[1]}\n")
                else:
                    history_questions.append(f"Q{i}. {conv['content']}\n")
        if ":" in doctor_question:
            question = doctor_question.split(':')[1]
        else:
            question = doctor_question
        system_prompt = f"""You are a patient interacting with a doctor. Instructions for Responding to Medical Questions:
Compare the doctor's current question with doctor's history questions, and determine whether the current question is a repeat of a previously asked question (has the similar meaning). 
If it is a repeat, please respond with "Sorry, you've asked this question before." 
If it is not a repeat, please respond with "OK."
OUTPUT FORMAT: <think>[Your thoughts here]</think><answer>[Your response here]</answer>."""
        prompt = f""" 
Doctor's History questions:
[
{"".join(history_questions)}
]

Doctor's Current Question: {question}
"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]

        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        return prompt + "<think>"

    def _prepare_patient_prompt(self, doctor_question):
        """
        Prepare a prompt for the environment LLM based on the doctor's question.
        
        Args:
            doctor_question: The doctor's question
            
        Returns:
            A formatted prompt for the environment LLM
        """
        system_prompt = f"""You are a patient interacting with a doctor. Instructions for Responding to Medical Questions:
Answer each medical question from the doctor concisely in a single sentence, strictly describing your symptoms and avoiding any mention of diagnoses and recommendations.
If the question is unrelated to your self-report states: "Sorry, I cannot answer your question."

Your self-report states: {self.description}
"""
        if ":" in doctor_question:
            question = doctor_question.split(':')[1]
        else:
            question = doctor_question
        prompt = f"""{question}"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]

        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        return prompt
    
    def render(self, mode: str = 'rgb_array') -> str:
        """
        Render the current state of the environment.
        
        This function formats the patient's response to provide the actor with
        the current state of the medical consultation.
        
        Args:
            mode: Rendering mode (only 'rgb_array' is supported)
            
        Returns:
            A string representation of the current state
        """
        if mode != 'rgb_array': # keep consistent with the original ragen
            raise ValueError(f"Unsupported render mode: {mode}")
            
        # 构建输出
        output = ""
        
        # 只显示患者当前这一轮的回复
        if self.conversation_history and len(self.conversation_history) >= 2:
            # 获取最后一轮对话（医生的问题和患者的回复）
            last_doctor_turn = self.conversation_history[-2]
            last_patient_turn = self.conversation_history[-1]
            
            # 只显示患者的回复
            output += f"{last_patient_turn['content']}\n"
        else:
            output += "No patient response yet.\n"
        
        # 添加诊断状态
        if self.diagnosis_made:
            output += "\nDiagnosis has been made. Consultation is complete."
        else:
            output += f"\nTurn {self.current_turn}/{self.max_turns}. "
            # if self.current_turn >= self.max_turns:
            #     output += "Maximum turns reached. Please make a diagnosis."
            # else:
            #     output += "Please continue asking questions or make a diagnosis."

        return output
    
    def success(self) -> bool:
        """
        Check if the diagnosis was correct.
        """
        if not self.diagnosis_made:
            return False
        return self.reward >= 10.0
    
    def finished(self) -> bool:
        """
        Check if the consultation is finished.
        """
        return self.diagnosis_made # if diagnosis is made, the episode is finished
    
    def copy(self) -> 'MedicalConsultationEnvWithPatientLLM':
        """
        Create a deep copy of the environment.
        Only copy instance-specific data, share static data.
        """
        new_env = MedicalConsultationEnvWithPatientLLM(
            parquet_path=self.parquet_path,
            env_llm_worker=self.env_llm_worker,
            tokenizer=self.tokenizer,
            max_turns=self.max_turns
        )
        
        # 只复制实例特定的数据
        new_env.conversation_history = self.conversation_history.copy()
        new_env.diagnosis_made = self.diagnosis_made
        new_env.index = self.index
        new_env.current_turn = self.current_turn
        new_env.description = self.description

        self._copy_tracking_variables(new_env)
        return new_env
    
    def extract_action(self, text: str) -> str:
        """
        Extract action from text input.
        """
        return text.strip() 

    def _calculate_recall_and_precision(self):
        """
        Calculate recall and precision for the current episode.
        """
        # Assume candidate_questions and conversation_history are attributes of self
        candidate_questions = [" ".join(q['doctor_question']) for q in self.candidate_questions]
        predict_questions = [conv['content'] for conv in self.conversation_history if conv['role'] == 'patient']

        # Calculate the number of relevant items
        relevant_count = len(candidate_questions)
        # Calculate the number of retrieved items (all questions asked)
        retrieved_count = len(predict_questions)
        # Calculate the number of relevant and retrieved items
        relevant_retrieved_count = 0
        unique_relevant_questions = set()
        for question in predict_questions:
            if question in candidate_questions and question not in unique_relevant_questions:
                relevant_retrieved_count += 1
                unique_relevant_questions.add(question)

        # Calculate recall and precision
        if relevant_count == 0:
            recall = 0
        else:
            recall = relevant_retrieved_count / relevant_count

        if retrieved_count == 0:
            precision = 0
        else:
            precision = relevant_retrieved_count / retrieved_count

        return recall, precision

    @staticmethod
    def formulate_output(env_feedback: str, done: bool = False, response: str = None):
        """
        Formulate the environment feedback to as the input to the LLM
        - e.g., For Qwen, special tokens like <|im_start|>user and <|im_end|> should be added
        NOTE hard-coded now for Qwen
        """

        if response is not None and "<|im_end|>" not in response:
            output = "<|im_end|>\n <|im_start|>user\n" + env_feedback + "<|im_end|>\n"
        else:
            output = "\n <|im_start|>user\n" + env_feedback + "<|im_end|>\n"
        if not done:
            output += "<|im_start|>assistant\n<think>"
        return output
    
    @classmethod
    def execute_predictions(cls, envs: List['MedicalConsultationEnvWithPatientLLM'], predictions: List[str], prediction_ids: torch.Tensor, tokenizer: AutoTokenizer):
        """
        Execute predictions across multiple environments with batch LLM processing.
        
        Args:
            envs: List of environment instances
            predictions: List of action predictions
            prediction_ids: Tensor of prediction IDs
            tokenizer: Tokenizer for processing text
            
        Returns:
            List of observation strings and done flags
        """
        cur_actions, action_is_valid = cls.postprocess_predictions(envs, predictions)
        
        # 初始化结果列表，长度与输入环境数量相同
        next_obs = [""] * len(envs)
        dones = [False] * len(envs)
        
        # 收集需要 LLM 处理的环境和动作
        llm_envs = []
        llm_actions = []
        llm_indices = []
        
        for i, (env, action, response, response_id, av) in enumerate(zip(envs, cur_actions, predictions, prediction_ids, action_is_valid)):
            env.current_turn += 1
            reward = 0

            if env.finished():
                if "<|im_end|>" not in response:
                    obs = "<|im_end|>"
                obs += tokenizer.pad_token
                done = True
                next_obs[i] = obs
                dones[i] = done
                continue

            count_answer = response.count("</answer>")
            match = re.search(r"</answer>(.*?)(<\|im_end\|>|$)", response, re.DOTALL)
            # 处理无效动作
            if not av or count_answer != 1 or (match and len(match.group(1)) != 0):
                # obs = "Your question is invalid"
                obs = "Your format is invalid. Please strictly follow the format: <think>[Thinking]</think><answer>[Question]</answer>. Do not output any other content after </answer>."
                reward += -2.0
                done = False

                # 更新对话历史
                env.conversation_history.append({"role": "doctor", "content": response})
                env.conversation_history.append({"role": "patient", "content": obs})
                
                # 更新跟踪变量
                env._update_tracking_variables(
                    response=response,
                    action=action,
                    action_is_valid=False,
                    action_is_effective=False,
                    reward=reward
                )
                
                # 生成观察
                obs = cls.formulate_output(env.render(), done, response)
                next_obs[i] = obs
                dones[i] = done
                continue
            
            # 检查是否是诊断动作
            # if "<diagnosis>" in action:
            if "Diagnosis" in action:
                # 提取诊断内容
                # diagnosis_match = re.search(r"<diagnosis>(.*?)</diagnosis>", action)
                diagnosis_match = re.search(r"Diagnosis[:：](.*?)(?=Recommendation[:：]|$)", action, re.DOTALL)
                if diagnosis_match:
                    diagnosis = diagnosis_match.group(1).strip()
                    env.diagnosis_made = True
                    
                    # 获取真实诊断
                    gt_diagnosis = cls._shared_data[env.index]['target']['diagnosis']
                    
                    # 计算诊断的 similarity 分数
                    similarity = env._get_rouge_score(diagnosis, gt_diagnosis)
                    reward += similarity * 5  # 诊断奖励
                    env.diagnosis_score = similarity
                    assert similarity >= 0
                    
                    # 检查是否有建议
                    # suggestion_match = re.search(r"<recommendation>(.*?)</recommendation>", action)
                    suggestion_match = re.search(r"Recommendation[:：](.*?)(?=\n|$)", action, re.DOTALL)
                    if suggestion_match:
                        suggestion = suggestion_match.group(1).strip()
                        gt_suggestion = cls._shared_data[env.index]['target']['recommendation']
                        if len(gt_suggestion) > 0:
                            similarity = env._get_rouge_score(suggestion, gt_suggestion)
                            reward += similarity * 5  # 建议奖励
                            env.recommandation_score = similarity
                            assert similarity >= 0
                    
                    # 更新跟踪变量
                    env._update_tracking_variables(
                        response=response,
                        action=action,
                        action_is_valid=av,
                        action_is_effective=True,
                        reward=reward
                    )
                    
                    # 生成观察
                    obs = cls.formulate_output(env.render(), True, response)
                    next_obs[i] = obs
                    dones[i] = True
                    continue
            
            if env.current_turn >= env.max_turns:
                obs = "Maximum turns reached. Please make a diagnosis."
                reward += -5.0
                done = True

                # 更新对话历史
                env.conversation_history.append({"role": "doctor", "content": response})
                env.conversation_history.append({"role": "patient", "content": obs})

                # 更新跟踪变量
                env._update_tracking_variables(
                    response=response,
                    action=action,
                    action_is_valid=av,
                    action_is_effective=False,
                    reward=reward
                )

                # 生成观察
                obs = cls.formulate_output(env.render(), done, response)
                next_obs[i] = obs
                dones[i] = done
                continue

            count_question = action.count("?") + action.count("？")
            if count_question != 1:
                obs = "You can only ask one question at a time."
                reward += -2.0
                done = False

                # 更新对话历史
                env.conversation_history.append({"role": "doctor", "content": response})
                env.conversation_history.append({"role": "patient", "content": obs})

                # 更新跟踪变量
                env._update_tracking_variables(
                    response=response,
                    action=action,
                    action_is_valid=False,
                    action_is_effective=False,
                    reward=reward
                )

                # 生成观察
                obs = cls.formulate_output(env.render(), done, response)
                next_obs[i] = obs
                continue
            
            # 收集需要 LLM 处理的环境
            llm_envs.append(env)
            llm_actions.append(action)
            llm_indices.append(i)
        
        # 批量处理需要 LLM 的环境
        if llm_envs:
            # Firstly 判断问题是否重复
            # 准备批量提示
            batch_prompts = []
            for env, action in zip(llm_envs, llm_actions):
                prompt = env._prepare_judge_prompt(action)
                batch_prompts.append(prompt)
            
            # 创建批量 DataProto
            tokenizer.padding_side = 'left'
            batch_encodings = tokenizer(batch_prompts, padding=True, truncation=True, return_tensors='pt')

            # Compute position_ids from attention_mask
            position_ids = compute_position_id_with_mask(batch_encodings['attention_mask'])

            batch_data = DataProto.from_dict({
                'input_ids': batch_encodings['input_ids'],
                'attention_mask': batch_encodings['attention_mask'],
                'position_ids': position_ids
            })
            batch_data.meta_info = {
                'eos_token_id': tokenizer.eos_token_id,
                'pad_token_id': tokenizer.pad_token_id,
                'recompute_log_prob': False,
                'do_sample': False,
                'validate': True,
            }
            # 处理GPU填充
            batch_responses = cls._handle_gpu_padding(llm_envs[0].env_llm_worker, batch_data)
            for k, v in batch_responses.batch.items():
                if isinstance(v, torch.Tensor):
                    if v.dtype != torch.int64:
                        batch_responses.batch[k] = v.to(torch.int64)
            batch_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch_responses.batch['responses']]

            llm_envs_valid = []
            llm_actions_valid = []
            llm_indices_valid = []
            for i, (env_idx, response_text) in enumerate(zip(llm_indices, batch_texts)):
                env = llm_envs[i]
                action = llm_actions[i]
                response = predictions[env_idx]
                av = action_is_valid[env_idx]
                # 检查是否是重复问题
                if "Sorry" in response_text:
                    obs = "You've asked this question before. Please ask a new question."
                    reward = -2.0
                    done = False

                    # 更新对话历史
                    env.conversation_history.append({"role": "doctor", "content": response})
                    env.conversation_history.append({"role": "patient", "content": obs})

                    # 更新跟踪变量
                    env._update_tracking_variables(
                        response=response,
                        action=action,
                        action_is_valid=av,
                        action_is_effective=False,
                        reward=reward
                    )
                    # 生成观察
                    obs = cls.formulate_output(env.render(), done, response)
                    next_obs[env_idx] = obs
                    dones[env_idx] = done
                else:
                    llm_envs_valid.append(env)
                    llm_actions_valid.append(action)
                    llm_indices_valid.append(env_idx)

            llm_envs = llm_envs_valid
            llm_actions = llm_actions_valid
            llm_indices = llm_indices_valid
            # 准备批量提示
            batch_prompts = []
            for env, action in zip(llm_envs, llm_actions):
                prompt = env._prepare_patient_prompt(action)
                batch_prompts.append(prompt)
            
            # 创建批量 DataProto
            tokenizer.padding_side = 'left'
            batch_encodings = tokenizer(batch_prompts, padding=True, truncation=True, return_tensors='pt')

            # Compute position_ids from attention_mask
            position_ids = compute_position_id_with_mask(batch_encodings['attention_mask'])

            batch_data = DataProto.from_dict({
                'input_ids': batch_encodings['input_ids'],
                'attention_mask': batch_encodings['attention_mask'],
                'position_ids': position_ids
            })
            batch_data.meta_info = {
                'eos_token_id': tokenizer.eos_token_id,
                'pad_token_id': tokenizer.pad_token_id,
                'recompute_log_prob': False,
                'do_sample': False,
                'validate': True,
            }
            # 处理GPU填充
            batch_responses = cls._handle_gpu_padding(llm_envs[0].env_llm_worker, batch_data)
            for k, v in batch_responses.batch.items():
                if isinstance(v, torch.Tensor):
                    if v.dtype != torch.int64:
                        batch_responses.batch[k] = v.to(torch.int64)
            batch_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch_responses.batch['responses']]
            
            # 处理每个环境的响应
            for i, (env_idx, response_text) in enumerate(zip(llm_indices, batch_texts)):
                reward = 0
                env = llm_envs[i]
                action = llm_actions[i]
                response = predictions[env_idx]
                av = action_is_valid[env_idx]
                
                # Extract only the response part after the assistant marker
                if "\nassistant\n" in response_text:
                    response_text = response_text.split("\nassistant\n")[1].strip()

                if "Sorry" in response_text:
                    reward += -1.0
                    patient_response = "Sorry, I do not know."
                else:
                    patient_response = response_text
                    reward += 1.0
                
                # 更新对话历史
                env.conversation_history.append({"role": "doctor", "content": action})
                env.conversation_history.append({"role": "patient", "content": patient_response})
                
                done = env.finished()
                
                # 更新跟踪变量
                env._update_tracking_variables(
                    response=response,
                    action=action,
                    action_is_valid=av,
                    action_is_effective=reward > 0,
                    reward=reward
                )
                
                # 生成观察
                obs = cls.formulate_output(env.render(), done, response)
                next_obs[env_idx] = obs
                dones[env_idx] = done
        
        return next_obs, dones
        
    @classmethod
    def _handle_gpu_padding(cls, env_llm_worker, batch_data):
        """
        处理GPU填充，确保批次大小能被GPU数量整除
        
        Args:
            env_llm_worker: 环境LLM工作器
            batch_data: 批次数据
            
        Returns:
            处理后的批次响应
        """
        # 获取GPU数量
        num_gpus = env_llm_worker._world_size
        if num_gpus <= 1:
            return env_llm_worker.generate_responses(batch_data)
            
        batch_size = batch_data.batch['input_ids'].shape[0]
        remainder = batch_size % num_gpus
        
        if remainder == 0:
            return env_llm_worker.generate_responses(batch_data)
            
        # 添加填充序列
        padding_size = num_gpus - remainder
        padded_batch = {}
        
        for k, v in batch_data.batch.items():
            # 使用第一个序列作为填充模板
            pad_sequence = v[0:1].repeat(padding_size, *[1] * (len(v.shape) - 1))
            padded_batch[k] = torch.cat([v, pad_sequence], dim=0)
            
        padded_batch_data = DataProto.from_dict(padded_batch)
        padded_batch_data.meta_info = batch_data.meta_info
        
        # 使用填充批次生成
        padded_output = env_llm_worker.generate_responses(padded_batch_data)
        
        # 从输出中移除填充
        trimmed_batch = {}
        for k, v in padded_output.batch.items():
            if isinstance(v, torch.Tensor):
                if v.dtype != torch.int64:
                    v = v.to(torch.int64)
                trimmed_batch[k] = v[:-padding_size]
            else:
                trimmed_batch[k] = v
        
        # 处理meta_info（如果存在）
        if hasattr(padded_output, 'meta_info') and padded_output.meta_info:
            trimmed_meta = {}
            for k, v in padded_output.meta_info.items():
                if isinstance(v, torch.Tensor):
                    trimmed_meta[k] = v[:-padding_size]
                else:
                    trimmed_meta[k] = v
            padded_output.meta_info = trimmed_meta
            
        padded_output.batch = trimmed_batch
        return padded_output

    def _get_rouge_score(self, text1, text2):
        from collections import Counter
        import jieba

        if not text1 or not text2:
            return 0.0

        words1 = jieba.lcut(text1)
        words2 = jieba.lcut(text2)

        count1 = Counter(words1)
        count2 = Counter(words2)

        common = count1 & count2
        num_common = sum(common.values())

        if not words1 or not words2:
            return 0.0

        precision = num_common / len(words1) if len(words1) > 0 else 0.0
        recall = num_common / len(words2) if len(words2) > 0 else 0.0

        if precision + recall == 0:
            f1 = 0.0
        else:
            f1 = 2 * precision * recall / (precision + recall)

        return f1