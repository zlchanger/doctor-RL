from typing import Optional, List, Tuple, Any, Dict
from ragen.env.base import BaseLanguageBasedEnv
import random
import gymnasium as gym
import pandas as pd
import re
from rouge_score import rouge_scorer
import json
import torch
from verl import DataProto
from verl.utils.model import compute_position_id_with_mask
from transformers import AutoTokenizer
from ragen.workers.env_llm_worker import EnvironmentLLMWorker
from omegaconf import OmegaConf
import ray
import numpy as np

class MedicalConsultationEnv(BaseLanguageBasedEnv, gym.Env):
    """
    A medical consultation environment where a doctor (policy model) interacts with a patient (fixed LLM).
    The doctor asks questions and makes a diagnosis, while the patient responds based on their condition.
    The patient's responses are generated by an external LLM and passed to the environment.
    """

    INVALID_ACTION = "No question."
    
    # 类级别的共享数据
    _shared_data = None
    _shared_seed_to_index = None
    _parquet_path = None

    def __init__(self, parquet_path: str, env_llm_worker=None, tokenizer=None, max_turns=5):
        """
        Initialize the environment for Medical Consultation

        Args:
            parquet_path: Path to the parquet file containing patient data
            env_llm_worker: Worker for generating patient responses
            tokenizer: Tokenizer for the environment LLM
        """
        BaseLanguageBasedEnv.__init__(self)
        
        # 只在第一次初始化时加载数据
        if MedicalConsultationEnv._shared_data is None:
            MedicalConsultationEnv._shared_data, MedicalConsultationEnv._shared_seed_to_index = self._get_data_from_parquet(parquet_path)
            MedicalConsultationEnv._parquet_path = parquet_path
        
        # 实例级别的变量
        self.parquet_path = parquet_path
        self.conversation_history = []
        self.diagnosis_made = False
        self.max_turns = max_turns  # Maximum number of questions allowed
        self.index = None  # index of the data
        self.candidate_questions = None  # candidate questions (from the gt)
        self.visited_questions = []  # visited questions
        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True) 
        self.env_llm_worker = env_llm_worker
        self.tokenizer = tokenizer
        self.current_turn = 0

    @classmethod
    def clear_shared_data(cls):
        """Clear shared data when needed"""
        cls._shared_data = None
        cls._shared_seed_to_index = None
        cls._parquet_path = None

    @classmethod
    def get_shared_data_size(cls) -> int:
        """Get the size of shared data in memory"""
        if cls._shared_data is None:
            return 0
        import sys
        return sys.getsizeof(cls._shared_data) + sys.getsizeof(cls._shared_seed_to_index)

    @staticmethod
    def _get_data_from_parquet(path: str):
        """
        Get data from parquet file and create mapping.

        Args:
            path: Path to the parquet file containing the data

        Returns:
            tuple: (data, mapping) where
                data: List of dicts containing target and numbers for each problem
                mapping: Dict mapping original indices to new sequential indices

        The function:
        1. Reads the parquet file
        2. Extracts target numbers and available numbers for each problem
        3. Creates an index mapping to maintain reference to original data
        """
        df = pd.read_parquet(path)
        
        # Extract target and numbers for each problem
        data = []
        for item in df.reward_model.values:
            data.append(
                {
                    'target': item['ground_truth'],
                    'patient_information': item['patient_information']
                }
            )

        # Create mapping from original indices to sequential indices
        original_indices = [item['index'] for item in df.extra_info.values]
        seed_to_index = {orig_idx: new_idx for new_idx, orig_idx in enumerate(original_indices)}
        
        return data, seed_to_index
    
    
    def reset(self, seed: int = None) -> str:
        """Reset the environment and reward distributions"""
        gym.Env.reset(self, seed=seed)
            
        # Reset tracking variables
        self._reset_tracking_variables()
        self.index = self._shared_seed_to_index[seed]
        self.candidate_questions = self._shared_data[self.index]['patient_information']
        # 过滤一下self.candidate_questions，保留'patient_response'和'doctor_question'同时存在的
        self.candidate_questions = [q for q in self.candidate_questions if "patient_response" in q and "doctor_question" in q and isinstance(q['patient_response'], np.ndarray) and isinstance(q['doctor_question'], np.ndarray)]
        self.visited_questions = []  # index of the visited questions
        self.diagnosis_made = False
        self.current_turn = 0
        self.conversation_history = []
        return self.render()
    
    def step(self, action: str) -> Tuple[str, float, bool, Dict]:
        """
        Execute one step in the environment.
        
        Args:
            action: The doctor's question or diagnosis
            
        Returns:
            observation (rendered environment), reward, done, info
        """
        if self.diagnosis_made:
            return self.render(), 0, True, {"action_is_effective": False}
        
        reward = -0.5  # Penalty for each turn
        self.current_turn += 1
            
        # Check if the action is a diagnosis
        if "<diagnosis>" in action:
            # Match the tag and extract the diagnosis
            diagnosis = re.search(r"<diagnosis>(.*?)</diagnosis>", action).group(1)
            self.diagnosis_made = True
            # GT diagnosis
            gt_diagnosis = self._shared_data[self.index]['target']['diagnosis']
            # Calculate the similarity score based on Longest Common Subsequence (LCS)
            similarity = self._get_rouge_score(diagnosis, gt_diagnosis)
            reward = similarity * 10  # Scale the reward to 10

            # GT Suggestion
            gt_suggestion = self._shared_data[self.index]['target']['recommendation']
            if len(gt_suggestion) > 0:
                suggestion = re.search(r"<recommendation>(.*?)</recommendation>", action).group(1)
                similarity = self._get_rouge_score(suggestion, gt_suggestion)
                reward += similarity * 10  # Scale the reward to 10

            return self.render(), reward, True, {"action_is_effective": True}
        
        # For questions, generate patient response using the environment LLM
        patient_response = None
        question_idx = -1  # 默认问题编号为-1，表示没有匹配
        
        if self.env_llm_worker and self.tokenizer:
            # Prepare prompt for the environment LLM
            prompt = self._prepare_patient_prompt(action)
            
            # Create DataProto for the prompt
            prompt_data = DataProto.from_dict({
                'input_ids': self.tokenizer(prompt, return_tensors='pt')['input_ids'],
                'attention_mask': self.tokenizer(prompt, return_tensors='pt')['attention_mask']
            })
            
            # Generate patient response using Ray remote call
            response_data = ray.get(self.env_llm_worker.generate_responses.remote(prompt_data))
            llm_response = self.tokenizer.decode(response_data.batch['responses'][0], skip_special_tokens=True)
            
            # Extract only the response part after the assistant marker
            if "\nassistant\n" in llm_response:
                llm_response = llm_response.split("\nassistant\n")[1].strip()
            
            # 从响应中提取问题编号
            answer_match = re.search(r"<answer>(\d+)</answer>", llm_response)
            if answer_match:
                question_idx = int(answer_match.group(1))
                
                # 如果匹配到有效的问题编号
                if question_idx >= 0 and self.candidate_questions is not None and question_idx < len(self.candidate_questions):
                    # 如果问题尚未被访问过
                    if question_idx not in self.visited_questions:
                        reward += 1.0  # 匹配到新的候选问题，奖励 +1
                        self.visited_questions.append(question_idx)  # 记录已访问的问题
                    else:
                        reward -= 1.0  # 重复问题，惩罚 -1
                    
                    # 使用匹配到的问题对应的患者回复
                    patient_response = " ".join(self.candidate_questions[question_idx]["patient_response"])
                else:
                    # 如果问题编号无效，使用默认回复
                    patient_response = "I don't know."
            else:
                # 如果没有找到问题编号，使用默认回复
                patient_response = "I don't know."
        else:
            # Fallback to predefined responses if no environment LLM worker
            patient_response = self._get_fallback_response(action)
            
        # Add the doctor's question and patient's response to the conversation history
        self.conversation_history.append({"role": "doctor", "content": action})
        self.conversation_history.append({"role": "patient", "content": patient_response})
        
        # Check if we've reached the maximum number of turns
        done = self.current_turn >= self.max_turns
        
        return self.render(), reward, done, {"action_is_effective": True}
        
    def _prepare_patient_prompt(self, doctor_question):
        """
        Prepare a prompt for the environment LLM based on the doctor's question.
        
        Args:
            doctor_question: The doctor's question
            
        Returns:
            A formatted prompt for the environment LLM
        """
        # Format the conversation history
        conversation_text = ""
        for turn in self.conversation_history:
            role = "Doctor" if turn["role"] == "doctor" else "Patient"
            conversation_text += f"{role}: {turn['content']}\n\n"
        
        # Format candidate questions
        candidate_questions_text = ""
        if self.candidate_questions is not None:
            candidate_questions_text = "Candidate questions:\n"
            for q_idx, q_item in enumerate(self.candidate_questions):
                candidate_questions_text += f"Question {q_idx}:\n"
                for q in q_item["doctor_question"]:
                    candidate_questions_text += f"  - {q}\n"
                patient_response = " ".join(q_item['patient_response'])
                candidate_questions_text += f"  Response: {patient_response}\n\n"
        
        # Create the prompt
        prompt = f"""<|im_start|>user\nYou are a patient answering a doctor's question.

Below are some candidate doctor questions and the corresponding patient responses.

Your task is to follow these steps:
1. Compare the doctor's new question with the candidate questions.
2. Find the most semantically similar candidate question (do not require exact wording).
3. Respond using the corresponding patient response.
4. Format your output like this:
   Response: [your patient response]
   <answer>N</answer>  # where N is the index number of the matched candidate question
5. If no candidate question is semantically similar, respond naturally as a patient and write <answer>-1</answer>.

Remember: Semantic similarity is based on meaning, not exact words.

---

Example:

Candidate questions:
Question 0:
  - How long has this been going on?
  Response: About two or three days. It's a dull pain that comes and goes.

Doctor's question: How long have you had this issue?

→ This is semantically similar to Question 0.

Response: About two or three days. It's a dull pain that comes and goes.
<answer>0</answer>

---

Now complete the following:

{candidate_questions_text}

---

Doctor's question: {doctor_question}

<|im_end|>
<|im_start|>assistant\n"""
        
        return prompt
        
    def _get_fallback_response(self, doctor_question):
        """
        Get a fallback response when the environment LLM is not available.
        
        Args:
            doctor_question: The doctor's question
            
        Returns:
            A predefined response
        """
        # Simple fallback responses based on keywords in the question
        if "pain" in doctor_question.lower():
            return "Yes, I've been experiencing pain for about a week now."
        elif "fever" in doctor_question.lower():
            return "I had a fever of 101°F yesterday, but it's gone down to 99°F now."
        elif "headache" in doctor_question.lower():
            return "I've had a headache for the past few days, especially in the morning."
        elif "sleep" in doctor_question.lower():
            return "I've been having trouble sleeping, maybe 4-5 hours a night."
        elif "appetite" in doctor_question.lower():
            return "My appetite has been reduced, I've lost about 5 pounds in the last month."
        else:
            return "I'm not sure how to answer that. Could you rephrase your question?"

    def _get_rouge_score(self, text1, text2):
        """
        Calculate similarity score based on Longest Common Subsequence (LCS).
        This is particularly suitable for Chinese text.
        
        Args:
            text1: First text
            text2: Second text
            
        Returns:
            LCS-based similarity score (normalized by the length of longer text)
        """
        def lcs_length(s1, s2):
            m, n = len(s1), len(s2)
            dp = [[0] * (n + 1) for _ in range(m + 1)]
            
            for i in range(1, m + 1):
                for j in range(1, n + 1):
                    if s1[i-1] == s2[j-1]:
                        dp[i][j] = dp[i-1][j-1] + 1
                    else:
                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])
            
            return dp[m][n]
        
        if not text1 or not text2:
            return 0.0
        
        lcs_len = lcs_length(text1, text2)
        # Normalize by the length of the longer text
        max_len = max(len(text1), len(text2))
        
        return lcs_len / max_len if max_len > 0 else 0.0
    
    def render(self, mode: str = 'rgb_array') -> str:
        """
        Render the current state of the environment.
        
        This function formats the patient's response to provide the actor with
        the current state of the medical consultation.
        
        Args:
            mode: Rendering mode (only 'rgb_array' is supported)
            
        Returns:
            A string representation of the current state
        """
        if mode != 'rgb_array': # keep consistent with the original ragen
            raise ValueError(f"Unsupported render mode: {mode}")
            
        # 构建输出
        output = ""
        
        # 只显示患者当前这一轮的回复
        if self.conversation_history and len(self.conversation_history) >= 2:
            # 获取最后一轮对话（医生的问题和患者的回复）
            last_doctor_turn = self.conversation_history[-2]
            last_patient_turn = self.conversation_history[-1]
            
            # 只显示患者的回复
            output += f"{last_patient_turn['content']}\n"
        else:
            output += "No patient response yet.\n"
        
        # 添加诊断状态
        if self.diagnosis_made:
            output += "\nDiagnosis has been made. Consultation is complete."
        else:
            output += f"\nTurn {self.current_turn}/{self.max_turns}. "
            # if self.current_turn >= self.max_turns:
            #     output += "Maximum turns reached. Please make a diagnosis."
            # else:
            #     output += "Please continue asking questions or make a diagnosis."

        return output
    
    def success(self) -> bool:
        """
        Check if the diagnosis was correct.
        """
        if not self.diagnosis_made:
            return False
        return self.reward >= 7.0
    
    def finished(self) -> bool:
        """
        Check if the consultation is finished.
        """
        return self.diagnosis_made # if diagnosis is made, the episode is finished
    
    def copy(self) -> 'MedicalConsultationEnv':
        """
        Create a deep copy of the environment.
        Only copy instance-specific data, share static data.
        """
        new_env = MedicalConsultationEnv(
            parquet_path=self.parquet_path,
            env_llm_worker=self.env_llm_worker,
            tokenizer=self.tokenizer,
            max_turns=self.max_turns
        )
        
        # 只复制实例特定的数据
        new_env.conversation_history = self.conversation_history.copy()
        new_env.diagnosis_made = self.diagnosis_made
        new_env.candidate_questions = self.candidate_questions
        new_env.visited_questions = self.visited_questions.copy()
        new_env.index = self.index
        new_env.current_turn = self.current_turn

        self._copy_tracking_variables(new_env)
        return new_env
    
    def extract_action(self, text: str) -> str:
        """
        Extract action from text input.
        """
        return text.strip() 

    def _calculate_recall_and_precision(self):
        """
        Calculate recall and precision for the current episode.
        """
        # Assume candidate_questions and conversation_history are attributes of self
        candidate_questions = [" ".join(q['doctor_question']) for q in self.candidate_questions]
        predict_questions = [conv['content'] for conv in self.conversation_history if conv['role'] == 'patient']

        # Calculate the number of relevant items
        relevant_count = len(candidate_questions)
        # Calculate the number of retrieved items (all questions asked)
        retrieved_count = len(predict_questions)
        # Calculate the number of relevant and retrieved items
        relevant_retrieved_count = 0
        unique_relevant_questions = set()
        for question in predict_questions:
            if question in candidate_questions and question not in unique_relevant_questions:
                relevant_retrieved_count += 1
                unique_relevant_questions.add(question)

        # Calculate recall and precision
        if relevant_count == 0:
            recall = 0
        else:
            recall = relevant_retrieved_count / relevant_count

        if retrieved_count == 0:
            precision = 0
        else:
            precision = relevant_retrieved_count / retrieved_count

        return recall, precision
    
    @classmethod
    def execute_predictions(cls, envs: List['MedicalConsultationEnv'], predictions: List[str], prediction_ids: torch.Tensor, tokenizer: AutoTokenizer):
        """
        Execute predictions across multiple environments with batch LLM processing.
        
        Args:
            envs: List of environment instances
            predictions: List of action predictions
            prediction_ids: Tensor of prediction IDs
            tokenizer: Tokenizer for processing text
            
        Returns:
            List of observation strings and done flags
        """
        cur_actions, action_is_valid = cls.postprocess_predictions(envs, predictions)
        
        # 初始化结果列表，长度与输入环境数量相同
        next_obs = [""] * len(envs)
        dones = [False] * len(envs)
        
        # 收集需要 LLM 处理的环境和动作
        llm_envs = []
        llm_actions = []
        llm_indices = []
        
        for i, (env, action, response, response_id, av) in enumerate(zip(envs, cur_actions, predictions, prediction_ids, action_is_valid)):
            env.current_turn += 1

            obs = ""
            if "<|im_end|>" not in response:
                obs += "<|im_end|>"

            if env.finished():
                obs += tokenizer.pad_token
                done = True
                next_obs[i] = obs
                dones[i] = done
                continue
            
            count_answer = response.count("</answer>")
            match = re.search(r"</answer>(.*?)(<\|im_end\|>|$)", response, re.DOTALL)
            # 处理无效动作
            if not av or count_answer != 1 or (match and len(match.group(1)) != 0):
                # obs = "Your question is invalid"
                obs = "Your format is invalid. Please strictly follow the format: <think>[Thinking]</think><answer>[Question]</answer>. Do not output any other content after </answer>."
                reward = -2.0
                done = False
                
                # 更新跟踪变量
                env._update_tracking_variables(
                    response=response,
                    action=action,
                    action_is_valid=False,
                    action_is_effective=False,
                    reward=reward
                )
                
                # 生成观察
                obs = cls.formulate_output(obs, done)
                next_obs[i] = obs
                dones[i] = done
                continue
            
            # 检查是否是诊断动作
            # if "<diagnosis>" in action:
            if "Diagnosis" in action:
                # 结束之前算提问的recall和precision作为reward
                # 计算召回和准确率
                recall, precision = env._calculate_recall_and_precision()
                reward = recall * 5 + precision * 5 
                # if recall + precision > 0: 
                #     reward += 2 * recall * precision/(recall + precision) * 5
                # else:
                #     reward = 0.0
                # 提取诊断内容
                # diagnosis_match = re.search(r"<diagnosis>(.*?)</diagnosis>", action)
                diagnosis_match = re.search(r"Diagnosis[:：](.*?)(?=Recommendation[:：]|$)", action, re.DOTALL)
                if diagnosis_match:
                    diagnosis = diagnosis_match.group(1).strip()
                    env.diagnosis_made = True
                    
                    # 获取真实诊断
                    gt_diagnosis = cls._shared_data[env.index]['target']['diagnosis']
                    
                    # 计算诊断的 similarity 分数
                    similarity = env._get_rouge_score(diagnosis, gt_diagnosis)
                    reward += similarity * 5  # 诊断奖励
                    
                    # 检查是否有建议
                    # suggestion_match = re.search(r"<recommendation>(.*?)</recommendation>", action)
                    suggestion_match = re.search(r"Recommendation[:：](.*?)(?=\n|$)", action, re.DOTALL)
                    if suggestion_match:
                        suggestion = suggestion_match.group(1).strip()
                        gt_suggestion = cls._shared_data[env.index]['target']['recommendation']
                        if len(gt_suggestion) > 0:
                            similarity = env._get_rouge_score(suggestion, gt_suggestion)
                            reward += similarity * 5  # 建议奖励
                    
                    # 更新跟踪变量
                    env._update_tracking_variables(
                        response=response,
                        action=action,
                        action_is_valid=av,
                        action_is_effective=True,
                        reward=reward
                    )
                    
                    # 生成观察
                    obs = cls.formulate_output(env.render(), True)
                    next_obs[i] = obs
                    dones[i] = True
                    continue
            else:
                if env.current_turn >= env.max_turns:
                    # 结束之前算提问的recall和precision作为reward
                    # 计算召回和准确率
                    recall, precision = env._calculate_recall_and_precision()
                    reward = recall * 5 + precision * 5 
                    # if recall + precision > 0: 
                    #     reward += 2 * recall * precision/(recall + precision) * 5
                    # else:
                    #     reward = 0.0
                    obs = "Maximum turns reached. Please make a diagnosis."
                    reward = -5.0
                    done = True

                    # 更新跟踪变量
                    env._update_tracking_variables(
                        response=response,
                        action=action,
                        action_is_valid=av,
                        action_is_effective=False,
                        reward=reward
                    )

                    # 生成观察
                    obs = cls.formulate_output(obs, done)
                    next_obs[i] = obs
                    dones[i] = done
                    continue
            
            # 收集需要 LLM 处理的环境
            llm_envs.append(env)
            llm_actions.append(action)
            llm_indices.append(i)
        
        # 批量处理需要 LLM 的环境
        if llm_envs:
            # 准备批量提示
            batch_prompts = []
            for env, action in zip(llm_envs, llm_actions):
                prompt = env._prepare_patient_prompt(action)
                batch_prompts.append(prompt)
            
            # 创建批量 DataProto
            tokenizer.padding_side = "left"
            batch_encodings = tokenizer(batch_prompts, padding=True, truncation=True, return_tensors='pt')

            # Compute position_ids from attention_mask
            position_ids = compute_position_id_with_mask(batch_encodings['attention_mask'])

            batch_data = DataProto.from_dict({
                'input_ids': batch_encodings['input_ids'],
                'attention_mask': batch_encodings['attention_mask'],
                'position_ids': position_ids
            })
            batch_data.meta_info = {
                'eos_token_id': tokenizer.eos_token_id,
                'pad_token_id': tokenizer.pad_token_id,
                'recompute_log_prob': False,
                'do_sample': False,
                'validate': True,
            }
            # 处理GPU填充
            batch_responses = cls._handle_gpu_padding(llm_envs[0].env_llm_worker, batch_data)
            for k, v in batch_responses.batch.items():
                if isinstance(v, torch.Tensor):
                    if v.dtype != torch.int64:
                        batch_responses.batch[k] = v.to(torch.int64)
            batch_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch_responses.batch['responses']]
            
            # 处理每个环境的响应
            for i, (env_idx, response_text) in enumerate(zip(llm_indices, batch_texts)):
                env = llm_envs[i]
                action = llm_actions[i]
                response = predictions[env_idx]
                av = action_is_valid[env_idx]
                
                # Extract only the response part after the assistant marker
                if "\nassistant\n" in response_text:
                    response_text = response_text.split("\nassistant\n")[1].strip()
                
                # 从响应中提取问题编号
                question_idx = -1  # 默认问题编号为-1，表示没有匹配
                answer_match = re.search(r"<answer>(-?\d+)</answer>", response_text, re.DOTALL)
                if answer_match:
                    question_idx = int(answer_match.group(1))
                
                # 根据问题编号选择患者回复
                if question_idx >= 0 and env.candidate_questions is not None and question_idx < len(env.candidate_questions):
                    # 使用匹配到的问题对应的患者回复
                    patient_response = " ".join(env.candidate_questions[question_idx]["patient_response"])
                    
                    # 如果问题尚未被访问过
                    if question_idx not in env.visited_questions:
                        reward = 1  # 匹配到新的候选问题，奖励 +1 (基础奖励-0.5 + 匹配奖励+1.0)
                        env.visited_questions.append(question_idx)  # 记录已访问的问题
                    else:
                        reward = -2  # 重复问题，惩罚 -1 (基础奖励-0.5 + 重复惩罚-1.0)
                else:
                    # 如果没有匹配到问题或问题编号无效，使用默认回复
                    patient_response = "Sorry, I don't have the information to answer this question. Could you try a different question that might help with diagnosis?"
                    reward = 0.0  # 基础奖励
                    if len(env.conversation_history) > 0 and env._get_rouge_score(action, env.conversation_history[-2]['content']) > 0.8:
                        reward = -2.0 # 重复问题
                
                # 更新对话历史
                env.conversation_history.append({"role": "doctor", "content": action})
                env.conversation_history.append({"role": "patient", "content": patient_response})
                
                done = env.finished()
                
                # 更新跟踪变量
                env._update_tracking_variables(
                    response=response,
                    action=action,
                    action_is_valid=av,
                    action_is_effective=reward > 0,
                    reward=reward
                )
                
                # 生成观察
                obs = cls.formulate_output(env.render(), done)
                next_obs[env_idx] = obs
                dones[env_idx] = done
        
        return next_obs, dones
        
    @classmethod
    def _handle_gpu_padding(cls, env_llm_worker, batch_data):
        """
        处理GPU填充，确保批次大小能被GPU数量整除
        
        Args:
            env_llm_worker: 环境LLM工作器
            batch_data: 批次数据
            
        Returns:
            处理后的批次响应
        """
        # 获取GPU数量
        num_gpus = env_llm_worker._world_size
        if num_gpus <= 1:
            return env_llm_worker.generate_responses(batch_data)
            
        batch_size = batch_data.batch['input_ids'].shape[0]
        remainder = batch_size % num_gpus
        
        if remainder == 0:
            return env_llm_worker.generate_responses(batch_data)
            
        # 添加填充序列
        padding_size = num_gpus - remainder
        padded_batch = {}
        
        for k, v in batch_data.batch.items():
            # 使用第一个序列作为填充模板
            pad_sequence = v[0:1].repeat(padding_size, *[1] * (len(v.shape) - 1))
            padded_batch[k] = torch.cat([v, pad_sequence], dim=0)
            
        padded_batch_data = DataProto.from_dict(padded_batch)
        padded_batch_data.meta_info = batch_data.meta_info
        
        # 使用填充批次生成
        padded_output = env_llm_worker.generate_responses(padded_batch_data)
        
        # 从输出中移除填充
        trimmed_batch = {}
        for k, v in padded_output.batch.items():
            if isinstance(v, torch.Tensor):
                if v.dtype != torch.int64:
                    v = v.to(torch.int64)
                trimmed_batch[k] = v[:-padding_size]
            else:
                trimmed_batch[k] = v
        
        # 处理meta_info（如果存在）
        if hasattr(padded_output, 'meta_info') and padded_output.meta_info:
            trimmed_meta = {}
            for k, v in padded_output.meta_info.items():
                if isinstance(v, torch.Tensor):
                    trimmed_meta[k] = v[:-padding_size]
                else:
                    trimmed_meta[k] = v
            padded_output.meta_info = trimmed_meta
            
        padded_output.batch = trimmed_batch
        return padded_output

if __name__ == "__main__":
    # 导入必要的模块
    from transformers import AutoTokenizer
    from ragen.workers.env_llm_worker import EnvironmentLLMWorker
    from omegaconf import OmegaConf
    
    # 设置环境参数
    config = OmegaConf.load("config/env/medical_consultation.yaml")

    # 初始化tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.env_llm.model.path)
    
    # 初始化环境LLM worker
    env_llm_worker = EnvironmentLLMWorker(config=config.env_llm, role='env_llm')
    
    # 创建环境实例
    env = MedicalConsultationEnv(
        parquet_path=config.env.train_path,
        env_llm_worker=env_llm_worker,
        tokenizer=tokenizer
    )
    
    # 重置环境
    observation = env.reset(seed=1)
    print("Initial observation:")
    print(observation)
    print("\n" + "="*50 + "\n")
    
    # 测试提问
    # questions = [
    #     "你好，请问你的症状持续多久了?",
    #     "有没有用过什么药？",
    #     "还有其他症状吗？"
    # ]

    # 英文提问
    questions = [
        "Hello, how long have you been feeling sick?",
        "Have you taken any medicine?",
        "Do you have any other symptoms?"
    ]

    for i, question in enumerate(questions):
        print(f"Turn {i+1}: Doctor asks: {question}")
        observation, reward, done, info = env.step(question)
        print(f"Patient response: {observation}")
        print(f"Reward: {reward}")
        print(f"Done: {done}")
        print(f"Info: {info}")
        print("\n" + "="*50 + "\n")
    
    # 测试诊断
    # diagnosis = "<diagnosis>患者可能患有肠炎</diagnosis><recommendation>建议服用抗生素，多休息，多喝水</recommendation>"
    # 英文诊断
    diagnosis = "<diagnosis>The patient may have enteritis</diagnosis><recommendation>Recommend taking antibiotics, rest more, and drink more water</recommendation>"
    print(f"Doctor makes diagnosis: {diagnosis}")
    observation, reward, done, info = env.step(diagnosis)
    print(f"Final observation: {observation}")
    print(f"Final reward: {reward}")
    print(f"Done: {done}")
    print(f"Info: {info}")
    
    # 测试环境复制
    print("\nTesting environment copy:")
    env_copy = env.copy()
    print(f"Original env diagnosis_made: {env.diagnosis_made}")
    print(f"Copied env diagnosis_made: {env_copy.diagnosis_made}")
    print(f"Original env conversation history length: {len(env.conversation_history)}")
    print(f"Copied env conversation history length: {len(env_copy.conversation_history)}")
    
    # 测试批量处理
    print("\nTesting batch processing:")
    envs = [env.copy() for _ in range(3)]
    predictions = ["Hello, how long have you been feeling sick?", "Have you taken any medicine?", "Do you have any other symptoms?"]
    prediction_ids = torch.tensor([0, 1, 2])
    
    next_obs, dones = MedicalConsultationEnv.execute_predictions(envs, predictions, prediction_ids, tokenizer)
    
    print(f"Batch processing results:")
    for i, (obs, done) in enumerate(zip(next_obs, dones)):
        print(f"Environment {i}:")
        print(f"Observation: {obs}")
        print(f"Done: {done}")
        print()
