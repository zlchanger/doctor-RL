from typing import Optional, List, Tuple, Any, Dict
from ragen.env.base import BaseLanguageBasedEnv
import random
import gymnasium as gym
import pandas as pd
import re
from rouge_score import rouge_scorer
import json
import torch
from verl import DataProto
from verl.utils.model import compute_position_id_with_mask
from transformers import AutoTokenizer
from ragen.workers.env_llm_worker import EnvironmentLLMWorker
from omegaconf import OmegaConf
import ray
import numpy as np
from ragen.env.medical_consultation.env_patient_llm import MedicalConsultationEnvWithPatientLLM

class MedicalConsultationEnvWithPatientLLMandRM(MedicalConsultationEnvWithPatientLLM):
    """
    A medical consultation environment where a doctor (policy model) interacts with a patient (fixed LLM).
    The doctor asks questions and makes a diagnosis, while the patient responds based on their condition.
    The patient's responses are generated by an external LLM and passed to the environment.
    """

    PENALTY_FOR_INVALID = 0
    INVALID_FORMAT_MESSAGE = "Your format is invalid. Please strictly follow the format: <think>[Thinking]</think><answer>[Question]</answer>. Do not output any other content after </answer>."
    MAX_TURNS_REACHED_MESSAGE = "Maximum turns reached. Please make a diagnosis."
    ONE_QUESTION_AT_A_TIME_MESSAGE = "You can only ask one question at a time."
    DUPLICATE_QUESTION_MESSAGE = "You've asked this question before. Please ask a new question."
    UNKNOWN_RESPONSE_MESSAGE = "Sorry, I do not know."
    DIAGNOSIS_REWARD_SCALE = 5
    RECOMMENDATION_REWARD_SCALE = 5
    INVALID_ACTION_PENALTY = -2.0
    MAX_TURNS_PENALTY = -5.0
    DUPLICATE_QUESTION_PENALTY = -2.0
    UNKNOWN_RESPONSE_PENALTY = -1.0
    VALID_RESPONSE_REWARD = 1.0

    def __init__(self, parquet_path: str, env_llm_worker=None, tokenizer=None, max_turns=5):
        """
        Initialize the environment for Medical Consultation

        Args:
            parquet_path: Path to the parquet file containing patient data
            env_llm_worker: Worker for generating patient responses
            tokenizer: Tokenizer for the environment LLM
        """
        if max_turns == -1:
            # random select a turn from 2 to 10
            self.random_turn = True
        else:
            self.random_turn = False
        super().__init__(parquet_path, env_llm_worker, tokenizer, max_turns)
        self.description = None
        self.diagnosis_score = None
        self.recommandation_score = None

    def copy(self) -> 'MedicalConsultationEnvWithPatientLLMandRM':
        """
        Create a deep copy of the environment.
        Only copy instance-specific data, share static data.
        """
        new_env = MedicalConsultationEnvWithPatientLLMandRM(
            parquet_path=self.parquet_path,
            env_llm_worker=self.env_llm_worker,
            tokenizer=self.tokenizer,
            max_turns=self.max_turns
        )
        
        # 只复制实例特定的数据
        new_env.conversation_history = self.conversation_history.copy()
        new_env.diagnosis_made = self.diagnosis_made
        new_env.index = self.index
        new_env.current_turn = self.current_turn
        new_env.description = self.description

        self._copy_tracking_variables(new_env)
        return new_env

    def reset(self, seed: int = None) -> str:
        """Reset the environment and reward distributions"""
        gym.Env.reset(self, seed=seed)
        
        if self.random_turn:
            # random select a turn from 2 to 10
            self.max_turns = random.randint(2, 10) # TODO: make it configurable

        # Reset tracking variables
        self._reset_tracking_variables()
        self.index = self._shared_seed_to_index[seed]
        self.description = self._shared_data[self.index]['description']
        self.diagnosis_made = False
        self.current_turn = 0
        self.conversation_history = []
        self.diagnosis_score = 0
        self.recommandation_score = 0

        return self.render()

    def render(self, mode: str = 'rgb_array') -> str:
        """
        Render the current state of the environment.
        
        This function formats the patient's response to provide the actor with
        the current state of the medical consultation.
        
        Args:
            mode: Rendering mode (only 'rgb_array' is supported)
            
        Returns:
            A string representation of the current state
        """
        if mode != 'rgb_array': # keep consistent with the original ragen
            raise ValueError(f"Unsupported render mode: {mode}")
            
        # 构建输出
        output = ""
        
        # 只显示患者当前这一轮的回复
        if self.conversation_history and len(self.conversation_history) >= 2:
            # 获取最后一轮对话（医生的问题和患者的回复）
            last_doctor_turn = self.conversation_history[-2]
            last_patient_turn = self.conversation_history[-1]
            
            # 只显示患者的回复
            output += f"{last_patient_turn['content']}\n"
        else:
            output += "No patient response yet.\n"
        
        # 添加诊断状态
        if self.diagnosis_made:
            output = "\nDiagnosis has been made. Consultation is complete."
        else:
            output += f"\nTurn {self.current_turn}/{self.max_turns}. You have to give a diagnosis and a recommendation before the end of the conversation.\n"

        return output
    
    def _process_invalid_action(self, env: 'MedicalConsultationEnvWithPatientLLMandRM', response: str, action: str, reward: float):
        """Handles invalid doctor actions."""
        obs = self.INVALID_FORMAT_MESSAGE
        reward += self.INVALID_ACTION_PENALTY
        done = False
        self._update_conversation_history(env, response, obs)
        self._update_tracking_variables_env(env, response, action, False, False, reward)
        return self.formulate_output(env.render(), done, response), done

    def _process_max_turns(self, env: 'MedicalConsultationEnvWithPatientLLMandRM', response: str, action: str):
        """Handles the case when the maximum number of turns is reached."""
        obs = self.MAX_TURNS_REACHED_MESSAGE
        reward = self.MAX_TURNS_PENALTY
        done = True
        env.diagnosis_made = True
        self._update_conversation_history(env, response, obs)
        self._update_tracking_variables_env(env, response, action, True, False, reward)
        return self.formulate_output(env.render(), done, response), done

    def _process_invalid_question_count(self, env: 'MedicalConsultationEnvWithPatientLLMandRM', response: str, action: str):
        """Handles cases where the doctor asks more than one question."""
        obs = self.ONE_QUESTION_AT_A_TIME_MESSAGE
        reward = self.INVALID_ACTION_PENALTY
        done = False
        self._update_conversation_history(env, response, obs)
        self._update_tracking_variables_env(env, response, action, False, False, reward)
        return self.formulate_output(env.render(), done, response), done

    def _process_duplicate_question(self, env: 'MedicalConsultationEnvWithPatientLLMandRM', response: str, action: str, patient_response: str):
        """Handles duplicate questions asked by the doctor."""
        obs = self.DUPLICATE_QUESTION_MESSAGE
        reward = self.DUPLICATE_QUESTION_PENALTY
        done = False
        self._update_conversation_history(env, response, obs)
        self._update_tracking_variables_env(env, response, action, False, False, reward)
        return self.formulate_output(env.render(), done, response), done

    def _process_patient_response(self, env: 'MedicalConsultationEnvWithPatientLLMandRM', response: str, action: str, patient_response: str):
        """Processes a valid patient response."""
        reward = self.VALID_RESPONSE_REWARD if "Sorry" not in patient_response else self.UNKNOWN_RESPONSE_PENALTY
        patient_response = patient_response if "Sorry" not in patient_response else self.UNKNOWN_RESPONSE_MESSAGE
        self._update_conversation_history(env, action, patient_response)
        done = env.finished()
        self._update_tracking_variables_env(env, response, action, True, reward > 0, reward)
        return self.formulate_output(env.render(), done, response), done

    def _update_conversation_history(self, env: 'MedicalConsultationEnvWithPatientLLMandRM', doctor_content: str, patient_content: str):
        """Updates the conversation history."""
        env.conversation_history.append({"role": "doctor", "content": doctor_content})
        env.conversation_history.append({"role": "patient", "content": patient_content})

    def _update_tracking_variables_env(self, env: 'MedicalConsultationEnvWithPatientLLMandRM', response: str, action: str, action_is_valid: bool, action_is_effective: bool, reward: float):
        """Updates the tracking variables in the environment."""
        env._update_tracking_variables(
            response=response,
            action=action,
            action_is_valid=action_is_valid,
            action_is_effective=action_is_effective,
            reward=reward
        )

    def _prepare_diagnosis_scoring_prompt(self, diagnosis: str, gt_diagnosis: str):
        """
        Prepares the prompt for scoring the diagnosis.
        Args:
            diagnosis: The proposed diagnosis
            gt_diagnosis: The ground truth diagnosis
        Returns:
            The prepared prompt
        """
        with open('ragen/env/medical_consultation/evaluation/eval_prompt_template_v2.txt', 'r') as file:
            prompt = file.read()
            prompt = prompt.format(candidate=diagnosis, reference=gt_diagnosis)
        return prompt

    def _prepare_recommendation_scoring_prompt(self, recommendation: str, gt_recommendation: str):
        """
        prepares the prompt for scoring the recommendation.
        Args:
            recommendation: The proposed recommendation
            gt_recommendation: The ground truth recommendation
        Returns:
            The prepared prompt
        """
        with open('ragen/env/medical_consultation/evaluation/eval_prompt_template_v2.txt', 'r') as file:
            prompt = file.read()
            prompt = prompt.format(candidate=recommendation, reference=gt_recommendation)
        return prompt

    @classmethod
    def execute_predictions(cls, envs: List['MedicalConsultationEnvWithPatientLLMandRM'], predictions: List[str], prediction_ids: torch.Tensor, tokenizer: AutoTokenizer):
        """
        Execute predictions across multiple environments with batch LLM processing.

        Args:
            envs: List of environment instances
            predictions: List of action predictions
            prediction_ids: Tensor of prediction IDs
            tokenizer: Tokenizer for processing text

        Returns:
            List of observation strings and done flags
        """
        cur_actions, action_is_valid = cls.postprocess_predictions(envs, predictions)
        num_envs = len(envs)
        next_obs = [""] * num_envs
        dones = [False] * num_envs
        llm_inputs = []
        llm_env_indices = []
        diagnosis_actions = []
        diagnosis_env_indices = []

        # First pass: Process non-diagnosis actions and collect diagnosis actions
        for i, (env, action, response, av) in enumerate(zip(envs, cur_actions, predictions, action_is_valid)):
            env.current_turn += 1
            reward = 0

            if env.finished():
                next_obs[i] = cls.formulate_output(env.render() + tokenizer.pad_token, True, response)
                dones[i] = True
                continue

            count_answer = response.count("</answer>")
            match = re.search(r"</answer>(.*?)(<\|im_end\|>|$)", response, re.DOTALL)
            if not av or count_answer != 1 or (match and len(match.group(1)) != 0):
                next_obs[i], dones[i] = env._process_invalid_action(env, response, action, reward)
                continue

            if "Diagnosis" in action:
                diagnosis_actions.append(action)
                diagnosis_env_indices.append(i)
                continue

            if env.current_turn >= env.max_turns:
                next_obs[i], dones[i] = env._process_max_turns(env, response, action)
                continue

            count_question = action.count("?") + action.count("？")
            if count_question != 1:
                next_obs[i], dones[i] = env._process_invalid_question_count(env, response, action)
                continue

            llm_inputs.append(env._prepare_judge_prompt(action))
            llm_env_indices.append(i)

        # Batch process question validity
        valid_question_envs = []
        valid_question_actions = []
        valid_question_indices = []
        if llm_inputs:
            batch_responses = cls._batch_llm_inference(envs[0].env_llm_worker, tokenizer, llm_inputs)
            for i, (env_idx, response_text) in enumerate(zip(llm_env_indices, batch_responses)):
                env = envs[env_idx]
                action = cur_actions[env_idx]
                response = predictions[env_idx]
                if "Sorry" in response_text:
                    next_obs[env_idx], dones[env_idx] = env._process_duplicate_question(env, response, action, response_text)
                else:
                    valid_question_envs.append(env)
                    valid_question_actions.append(action)
                    valid_question_indices.append(env_idx)
        else:
            valid_question_envs = []
            valid_question_actions = []
            valid_question_indices = []

        # Batch process patient responses for valid questions
        patient_responses_map = {}
        if valid_question_envs:
            patient_prompts = [env._prepare_patient_prompt(action) for env, action in zip(valid_question_envs, valid_question_actions)]
            patient_responses = cls._batch_llm_inference(valid_question_envs[0].env_llm_worker, tokenizer, patient_prompts)
            for i, (env_idx, patient_response_text) in enumerate(zip(valid_question_indices, patient_responses)):
                env = valid_question_envs[i]
                action = valid_question_actions[i]
                response = predictions[env_idx]

                if "\nassistant\n" in patient_response_text:
                    patient_response_text = patient_response_text.split("\nassistant\n")[1].strip()

                next_obs[env_idx], dones[env_idx] = env._process_patient_response(env, response, action, patient_response_text)
                patient_responses_map[env_idx] = patient_response_text  # Store patient responses for potential later use

        # Batch process diagnosis actions
        if diagnosis_actions:
            diagnosis_prompts = []
            gt_diagnoses = []
            gt_recommendations = []
            local_diagnosis_env_indices = []

            for i, action in enumerate(diagnosis_actions):
                diagnosis_match = re.search(r"Diagnosis[:：](.*?)(?=Recommendation[:：]|$)", action, re.DOTALL)
                recommendation_match = re.search(r"Recommendation[:：](.*?)(?=\n|$)", action, re.DOTALL)

                if diagnosis_match:
                    diagnosis = diagnosis_match.group(1).strip()
                    recommendation = recommendation_match.group(1).strip() if recommendation_match else ""
                    env_index = diagnosis_env_indices[i]
                    gt_diagnosis = cls._shared_data[envs[env_index].index]['target']['diagnosis']
                    gt_recommendation = cls._shared_data[envs[env_index].index]['target']['recommendation']

                    diagnosis_prompts.append(envs[env_index]._prepare_diagnosis_scoring_prompt(diagnosis, gt_diagnosis))
                    diagnosis_prompts.append(envs[env_index]._prepare_recommendation_scoring_prompt(recommendation, gt_recommendation))
                    gt_diagnoses.append(gt_diagnosis)
                    gt_recommendations.append(gt_recommendation)
                    local_diagnosis_env_indices.append(env_index)

            if diagnosis_prompts:
                all_scores = cls._batch_llm_inference(envs[0].env_llm_worker, tokenizer, diagnosis_prompts)
                score_index = 0
                for i, env_idx in enumerate(local_diagnosis_env_indices):
                    env = envs[env_idx]
                    action = diagnosis_actions[i]
                    response = predictions[env_idx]
                    reward = 0
                    env.diagnosis_score = 0
                    env.recommandation_score = 0

                    try:
                        # extract score from <answer></answer>
                        score = all_scores[score_index]
                        score_match = re.search(r"<answer>(.*?)</answer>", score, re.DOTALL)
                        if score_match:
                            score = score_match.group(1).strip()
                        score_pattern = r'[0-5]'
                        extracted_numbers = re.findall(score_pattern, score, re.DOTALL)
                        if extracted_numbers:
                            score = extracted_numbers[0]
                        else:
                            score = '0'
                            print(f"Warning: Invalid diagnosis format in LLM output: '{all_scores[score_index]}'. Setting to 0.")
                        diagnosis_score = float(score) if score else 0
                        reward += diagnosis_score
                        env.diagnosis_score = diagnosis_score
                    except (ValueError, IndexError):
                        print(f"Warning: Could not parse diagnosis score from LLM output: {all_scores[score_index] if score_index < len(all_scores) else 'None'}")
                    score_index += 1

                    try:
                        score = all_scores[score_index]
                        score_match = re.search(r"<answer>(.*?)</answer>", score, re.DOTALL)
                        if score_match:
                            score = score_match.group(1).strip()
                        score_pattern = r'[0-5]'
                        extracted_numbers = re.findall(score_pattern, score, re.DOTALL)
                        if extracted_numbers:
                            score = extracted_numbers[0]
                        else:
                            score = '0'
                            print(f"Warning: Invalid diagnosis format in LLM output: '{all_scores[score_index]}'. Setting to 0.")
                        recommendation_score = float(score) if score else 0
                        reward += recommendation_score
                        env.recommandation_score = recommendation_score
                    except (ValueError, IndexError):
                        print(f"Warning: Could not parse recommendation score from LLM output: {all_scores[score_index] if score_index < len(all_scores) else 'None'}")
                    score_index += 1

                    env.diagnosis_made = True
                    env._update_tracking_variables_env(env, response, action, True, True, reward)
                    next_obs[env_idx] = cls.formulate_output(env.render(), True, response)
                    dones[env_idx] = True

        return next_obs, dones

    @classmethod
    def _batch_llm_inference(cls, llm_worker, tokenizer, prompts: List[str]):
        """Performs batch inference using the LLM worker."""
        tokenizer.padding_side = 'left'
        batch_encodings = tokenizer(prompts, padding=True, truncation=True, return_tensors='pt')
        position_ids = compute_position_id_with_mask(batch_encodings['attention_mask'])
        batch_data = DataProto.from_dict({
            'input_ids': batch_encodings['input_ids'],
            'attention_mask': batch_encodings['attention_mask'],
            'position_ids': position_ids
        })
        batch_data.meta_info = {
            'eos_token_id': tokenizer.eos_token_id,
            'pad_token_id': tokenizer.pad_token_id,
            'recompute_log_prob': False,
            'do_sample': False,
            'validate': True,
        }
        batch_responses = cls._handle_gpu_padding(llm_worker, batch_data)
        for k, v in batch_responses.batch.items():
            if isinstance(v, torch.Tensor) and v.dtype != torch.int64:
                batch_responses.batch[k] = v.to(torch.int64)
        return [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch_responses.batch['responses']]

def compute_position_id_with_mask(attention_mask: torch.Tensor) -> torch.Tensor:
    """Computes position IDs from attention mask."""
    position_ids = attention_mask.long().cumsum(-1) - 1
    position_ids.masked_fill_(attention_mask == 0, 0)
    return position_ids